
% To compile this document
% graphics.off();rm(list=ls());library('knitr');knit('testing-lab.Rnw'); purl('testing-lab.Rnw'); for(i in 1:2) system('R CMD pdflatex testing-lab.tex');



\documentclass{article}


<<style, echo=FALSE, results='asis'>>=
BiocStyle::latex()
@



<<options, include=FALSE>>=
options(digits=3, width=80)
opts_chunk$set(echo=TRUE,tidy=FALSE,include=TRUE,
               dev='pdf', fig.width = 6, fig.height = 3.5, comment = '  ', dpi = 300,
		cache = T, lazy.load = FALSE, background="grey93" )
@



\title{Statistical Testing, Including Multiple Testing and Filtering / Weighting}
\usepackage[T1]{fontenc}
%\usepackage{enumerate}
\usepackage{amsmath, amssymb, amstext}
\usepackage{natbib}
\usepackage{mathpazo}
\usepackage{soul}
\usepackage{cases}
\setlength{\parindent}{0cm}
\usepackage{xhfill}
\usepackage[labelformat=empty]{caption}
\providecommand{\abs}[1]{\left\lvert#1\right\rvert}

\author{Bernd Klaus$^1$ \\[1em]European Molecular Biology Laboratory (EMBL),\\ Heidelberg, Germany\\
\texttt{$^1$bernd.klaus@embl.de}}

\begin{document}

\maketitle


\tableofcontents

%--------------------------------------------------
\section{Required packages and other preparations} \label{sec:prep}
%--------------------------------------------------



%
<<required packages and data, echo = TRUE, message = FALSE>>=

set.seed(999)

library(gplots)
library(RColorBrewer)
library(ggplot2)
library(plyr)
library(dplyr)
library(magrittr)
library(tidyr)
library(mutoss)
library(qvalue)
library(st)
library(ALL)
library(hgu95av2.db)
library(genefilter)
library(IHW)
library("DESeq")
library("ggplot2")
library("methods")
library("airway")
library("DESeq2")
data("airway")

@
%


\section{Introduction to statistical hypothesis testing}

In science it is common to ask if two things are different
Are men taller than women? Is the risk of cancer different in smokers and non-smokers? Is
the probability of getting type II different for different genetic backgrounds? Is this
gene differentially expressed in cancer? When we make two measurements and compare, we
almost always see some difference. But will wee see it again if we measure again? If
someone else measures? Statistical testing can help us answer this question.

Here we deal with questions  related to the statistical testing
biological hypothesis. Does the mean gene expression over ALL patients
differ from that over AML patients? That is, does the mean gene expression
level differ between experimental conditions? Is the mean gene expression
different from zero?   How can it be tested whether the
frequencies of nucleotide sequences of two genes are
different?  What is the probability of a certain micro RNA to have more than a certain
number of purines?


Many population parameters are used to define
families of theoretical distributions. In any research (empirical) setting the
specific values of such parameters are unknown so that these must be estimated.
Once estimates are available it becomes possible to statistically test
biologically important hypotheses. This lab gives several basic
examples of statistical testing and some of its background.

As a conceptual example for a typical testing situation, let $\mu_0$ be a
number representing the hypothesized population mean by a
researcher on the basis of experience and knowledge from the field. With
respect to the population mean the null hypothesis can be formulated as
$H_0 : \mu = \mu_0$ and the alternative hypothesis as $H_1$ : $\mu \neq \mu_0$ .
These are two statements of which the latter is the opposite of the first: Either
$H_0$ or $H_1$ is true. The alternative hypothesis is true if $H_1 :
\mu < \mu_0$ or $H_1 : \mu > \mu_0$ holds true.
This type of alternative hypothesis is called ``two-sided''. In case
$H_1$ : $\mu > \mu_0$ or $H_1$ : $\mu < \mu_0$, it is called ``one-sided''.


Such a null hypothesis will be statistically tested against the alternative
using a suitable distribution of a statistic (e.g. standardized mean). After
conducting the experiment, the value of the statistic can be computed from
the data. By comparing the value of the statistic with its distribution, the
researcher draws a conclusion with respect to the null hypothesis: $H_0$ is
rejected or it is not. The probability to reject $H_0$, given the truth of $H_0$, is
called the significance level which is generally denoted by $\alpha$. We shall follow
the habit in statistics to use $\alpha = 0.05$, but it will be completely clear how to
adapt the procedure in case other significance levels are desired.

This workflow can be summarized as follows:



\begin{enumerate}
\item Set up hypothesis $H_0$ (that you want to reject)
\item  Find a test statistic $T$ that should be sensitive to
(interesting) deviations from $H_0$
\item  Figure out the  null distribution of $T$, the distribution of $T$
under the assumption that $H_0$ holds
\item  Compute the actual value of $T$ for the data at
hand
\item  Compute $p$--value = the probability of observing that
value, or more extreme, assuming the null distribution.
\item Test Decision: Rejection of $H_0$ - yes / no ?
\end{enumerate}



\section{The two group comparison as a fundamental example for testing}


Imagine a researcher who would like to compare the height of two plant varieties.
If she only takes one measurement of the plant height and observes a difference of say 2cm,
it is impossible to say whether this difference is due to natural variation.
On the other hand, if multiple plants of each variety are measured, and it turns out
that the height differences are always somewhere around 2cm, the observed difference
is less likely due to chance. This is illustrated in the figure below:
the difference is strong relative to the variability between the measurements.

The data might look like this:

<<plantData, cache=FALSE>>=

fBar <- function(x){
    tmp <- mean(x, na.rm = T)
    data.frame(ymax = tmp, y = tmp, ymin = tmp)
}

variety1 <- rnorm(10, mean = 5)
variety2 <- rnorm(10, mean = 7, sd = 1/sqrt(2))

plantData <- data.frame(height  = c(variety1,variety2) ,
                     variety = sort(rep(c("variety1", "variety2"),10)) )

head(plantData)



(ggplot(aes(x = variety, y = height, color = variety, shape = variety, size = 3),
        data = plantData)

    + geom_jitter(width = 0.3, height = 0)


    + scale_color_brewer(palette = "Set1")

    + ylab("height in cm")

    + geom_errorbar(stat = "summary", width = 0.4,
                    fun.data = fBar, size = 0.4))



@


\subsubsection*{Side Note: Technical vs. biological replicates}

When referring to replicates it is important to distinguish between biological and
technical replicates. Technical replicates refer to experimental samples isolated
from one biological sample, e.g. extracting RNA from the cells of a mouse and
then preparing 3 sequencing libraries from this while a biological replication means
extracting RNA from three different mice for the comparisons of interest.
It is not sufficient "to pipette an experiment again" since this is not biological,
but merely a ``technical" replication. In general, technical replicates
tend to show less variability than biological replicates, thus leading
to false positive results.

\subsection{How to test for differences via permutations}

The observed mean difference between the the two varieties
is \Sexpr{mean(variety2) - mean(variety1)} cm. How easy would it be
for a difference of \Sexpr{mean(variety2) - mean(variety1)}
cm minutes to occur just by chance?

To answer this, we suppose there really is no difference between
the two groups, that variety1 and variety2 are just labels. So what would
happen if we assign labels randomly? How often would a difference like
\Sexpr{mean(variety2) - mean(variety1)} cm occur?

We'll pool all twenty observations, randomly pick 10 of them to label
basic and label the rest extended, and compute the difference in means
between the two groups. We'll repeat that many times, say ten thousand, to
get the permutation distribution shown below. The observed statistic
\Sexpr{mean(variety2) - mean(variety1)} cm is also shown; the fraction
of the distribution to the right of that value
is the probability that random labeling would give a difference that
large.

\textbf{In a permutation test, we obtain the null distribution from the data,
rather than analytically as e.g. in a t--test.}

\subsection{Run the permutation test}



<< permTest, dependson="plantsExample">>=

## helper function to compute permutation p-value

mDiff <- function(data, group){
  data$group <- NULL
  data$group <- group
  tmp <-  data %>%
    group_by(group)  %>%
    summarize(m = mean(data, na.rm=TRUE))
  as.numeric(tmp[2,"m"] - tmp[1,"m"])
}


## function to compute the permutation test
permTestTwoGroups <- function(group1, group2,
                              twoSided = TRUE, permutations = 1e4){

  stopifnot(is.numeric(group1),  is.numeric(group2),
             length(group1) > 0,  length(group2) > 0,
             is.vector(group1), is.vector(group2),
            is.logical(twoSided))

  inputData <- data.frame(data  = c(group1, group2),
                     group = rep(c("group1", "group2"),
                                   c(length(group1), length(group2))))

  # compute the observed difference between the groups
  obsDiff  <-  mDiff(inputData, inputData$group)

  # compute sampling distribution and p--value
  samplingDist <- c(replicate(as.integer(permutations),
                              mDiff(inputData, sample(inputData$group))),
                              obsDiff)

  # compute two-sided p-value
  pvalP <- 2*min(1 - ecdf(samplingDist)(abs(obsDiff)),
                 ecdf(samplingDist)(abs(obsDiff)))

  # create plot of the sampling distribution
  samplingDistPlot <- (qplot(samplingDist, fill = I("orange4"),
                             main = "Sampling distribution mean difference",
                              binwidth = 0.1)
  +  geom_vline(xintercept = obsDiff, size = 2))

  return(list(
    samplingDist = samplingDist,
    samplingDistPlot = samplingDistPlot,
    obsDiff = obsDiff,
    pval = pvalP
  ))

}

@


We now compute a difference for each label permutation  and plot it. We compute
a \textbf{two-sided p-value} by looking how many of the computed differences are less
than the observed one of \textbf{mean(variety2) - mean(variety1)} (lower p--value) and
how many are greater than `\textbf{mean(variety2) - mean(variety1)}. The two--sided p--value
corresponds to a test of the null hypothesis that the mean difference between the two varieties
is different from zero.

The lower p--value corresponds to a test of the null hypothesis
that the mean difference is less than zero while the  upper
p--value corresponds to a test of the null hypothesis
that the mean difference is greater than zero. The lower and upper p--values
correspond to so--called \textbf{one--sided} tests.
In order to compute the two--sided p--value,
we compute both one--sided ones and then take twice the smaller one.



<<getPvalue, dependson="permTest", cache = TRUE >>=
## draw 1e4 times a permutation of the  sample labels  and compute the
## two sided p--value


testResult <- permTestTwoGroups(group1 = variety1,
                  group2 = variety2)

testResult$pval

testResult$samplingDistPlot

@


In this case, the probability, the p--value, is \Sexpr{testResult$pval}; it would be rare
for a difference this large to occur by chance. The distribution that is shown
in the figure is called a \textbf{sampling distribution}. It describes how the our
 \textbf{test statistic} would be distributed if the \textbf{null hypothesis} was true,
i.e. if there was no difference between the the two varieties. The lower the variability
of the data and the higher the sample size, the "thinner" the sampling distribution will be.

The p--value gives the probability to observe a difference of \Sexpr{testResult$obsDiff} or greater
assuming that the null hypothesis is true. If this probability is very low, we can be confident
that the null hypothesis is not true and thus the alternative is, i.e. that there is actually
a difference between the height of the two varieties.

The name ``permutation test" stems from the fact that we picked
$n_1$ observations without replacement to label as the first sample,
and labelled the others as the second sample. This is equivalent to randomly
permuting all labels, hence the name. If we use all possible permutations for the test,
the test is also called an exact test. However, this is computationally unfeasible
for large sample sizes.

\subsection{Summary: two--sample permutation test recipe}

\begin{enumerate} [label=(\emph{\alph*})]
 \item Pool the values of the two groups
 \item repeat a large number of  times (> 10 000)
    \begin{itemize}
    \item  Draw a resample of size $n_1$ without replacement

    \item  Use the remaining $n_2$ observations for the other sample

    \item  Calculate the difference in means, or another statistic that com-
    pares samples

    \item   Plot a histogram of the random statistic values; show the observed
    statistic.

  \item  Calculate the p--value as the fraction of times the random statistics exceed or equal the observed statistic
  \end{itemize}
\end{enumerate}


\section{The two sample $t$--test}

Instead of a permutation test, we can use a t--test to test the difference between the
two varieties. In contrast to the permutation test, the sampling distribution of the
mean is obtained analytically, via the assumption of a normal distribution for both
input groups. We will discuss the normal distribution next.

\subsection{The Normal Distribution}

The normal distribution is of key importance because it is assumed for many
data generating processes. Among other things, we will look at (reprocessed)
gene expression values than can be seen as realizations of a random variable $X$
having a normal distribution.

Equivalently, one says that the data values are members of a normally distributed
population with mean $\mu$ (mu) and variance $\sigma^2$ (sigma squared). It
is good custom to use Greek letters for population properties and $N ( \mu ,\sigma^2 )$
for the normal distribution. The value of the distribution function is given
by $P (X \leq x)$, the probability of the population to have values smaller than
or equal to $x$. Various properties of the normal distribution are illustrated
by the examples below.


\subsection{Example: Explore the Normal Distribution}

To view members of the normal distribution load the
\CRANpkg{TeachingDemos"} ` package and enter the the command \Rfunction{vis.normal()}
to launch an interactive display of  densities of the normal distribution,
i.e. bell-shaped curves.
The curves are symmetric around $\mu$ and attain a
unique maximum at $x = \mu$. If $x$ moves further away from the mean $\mu$, then
the curves moves to zero so that extreme values occur with small probability.
Move the mean and the standard deviation from the left to the right to
explore their effect on the shape of the normal distribution. In particular,
when the mean $\mu$ increases, then the distribution moves to the right. If $\sigma$
is small/large, then the distribution is steep/flat.

\subsection{The ALL data}

The ALL data consist of microarrays from 128 different individuals with
acute lymphoblastic leukemia (ALL). There are 95 samples with B-cell ALL
and 33 with T-cell ALL and because these are different tissues and quite
different diseases we consider them separately and focus on the
B-cell ALL tumors.

An interesting subset, with two groups having approximately the same
number of samples in each group, is the comparison of the B-cell tumors
found to carry the BCR/ABL mutation to those B-cell tumors with no
observed cytogenetic abnormalities. These samples are labeled BCR/ABL
and NEG in the mol.biol covariate. The BCR/ABL mutation, also known
as the Philadelphia chromosome, was the first cytogenetic aberration that
could be associated with the development of cancer, leading the way to the
current understanding of the disease. In tumors harboring the BCR/ABL
translocation a short piece of chromosome 22 is exchanged with a segment
of chromosome 9. As a consequence, a constitutively active fusion protein
is transcribed which acts as a potent mitogene, leading to uncontrolled cell
division.
Not all leukemia tumors carry the Philadelphia chromosome; there
are other mutations that can be responsible for neoplastic alterations of
blood cells, for instance a translocation between chromosomes 4 and 11
(ALL1/AF4).

From the data, we look at the expression of the gene BCL2. The following
code chunk shows the preprocessing of the data. We only select the B-Cell
tumors and focus on the ones with/without a translocation.

<<setupALL >>=

data("ALL")
bALL <- ALL[, substr(ALL$BT,1,1) == "B"]
fusALL <- bALL[, bALL$mol.biol %in% c("BCR/ABL", "NEG")]
fusALL$mol.biol <- factor(fusALL$mol.biol)
fusALL
sample_n(pData(fusALL), 10)

groupsALL <- fusALL$mol.biol
expALL <- exprs(fusALL)

anno_fusALL <- plyr::ddply(AnnotationDbi::select(hgu95av2.db,
                                  keys=rownames(expALL),
                                  columns = c("SYMBOL", "GENENAME", "ENSEMBL"),
                                  keytype="PROBEID"), "PROBEID", function(X){X[1,]})



@



Now the the sample group is coded in the vector \Rfunction{groupsALL}, the expression
data is in \Rfunction{expALL} and the annotation of the probes is in
\Rfunction{anno\textunderscore fusALL}

<< showALL, dependson="setupALL" >>=

head(groupsALL)
head(expALL[, 1:5])
head(anno_fusALL)


@



\subsection{Example: A Normal Model for gene expression of BCL2}

Here we look at the gene expression values for the gene BCL2, which is in row
1152 of the data set.
<< getBCL2 >>=
anno_fusALL[1152,]
@


This gene encodes an integral outer mitochondrial membrane protein
that blocks the apoptotic death of some cells such as lymphocytes.
Constitutive expression of BCL2 is thought to be the cause of
follicular lymphoma. We now develop a normal distribution model for
the translocation group of the ALL data.



Suppose that the expression  values of the ALL group
of gene BCL2  can be represented by $X$ which is distributed as
$N (8.6, 0.5 )$. From
the graph of its density function, it can be observed that it
is symmetric and bell-shaped around $\mu = 8.6$.

A density function may very well be seen as a histogram with
 arbitrarily small bars (intervals). The
probability that the expression values are less than 8 is
$P (X < 8) =$ \Rfunction{pnorm(8, 8.6, 0.5)} $=$ \Sexpr{pnorm(8, 8.6, 0.5)}.


The figure next to it illustrates the value 0.115 of the
\textbf{cumulative distribution function (cdf)}
at $x = 8$. It corresponds to the area of the green colored surface below the graph of the
density function in the figure.


<< BCL2-density-cdf, echo = TRUE >>=

f <-function(x){dnorm(x, 8.6, 0.5)}
F <-function(x){pnorm(x, 8.6, 0.5)}

x <- seq(6,11,0.01)
dataGG <- data.frame(x = x, y = f(x))
dataGG <- mutate(dataGG,  area = ifelse(x < 8, "in", "out"  ))

p<-qplot(data = dataGG, x = x, y = y, geom="line")
p<-p + geom_area(aes(ymax = y, fill = area)) +  guides(fill=FALSE)
p<-p + xlab("Gene-Expression") + ylab("density f") + annotate("text", x = 8, y = 0, label = "8")
p + labs(title = "P(X<=8)= 0.115") + scale_fill_brewer(palette = "Dark2")


dataGG = data.frame(x = x, y = F(x))
dataGG <- mutate(dataGG,  area = ifelse(x < 8, "in", "out"  ))

p<-qplot(data = dataGG, x = x, y = y, geom="line")
p<- p + annotate("text", x = 8, y = 0, label = "8")
p<- p	+ annotate("text",y = .16, x = 6, label = "0.115")

p <- p + geom_segment(y=0, yend = F(8), x=8, xend = 8, size = I(1))
p <- p + geom_segment(y=F(8), yend = F(8), x=6, xend = 8, size = I(1))
p + labs(title = "P(X<=8)= 0.115") + xlab("Gene-Expression") + ylab("distribution F")


@


The probability that the expression values
are greater than $9$ is $P (X \geq 9) =$


<< BCL2-1, echo = TRUE >>=
1 - pnorm(9, 8.6, 0.5)
@


The probability that $X$ is between $8$ and $9$ equals
$P (8 \leq X \leq 9) =$


<< BCL2-2, echo = TRUE >>=
pnorm(9, 8.6, 0.5) - pnorm(8, 8.6, 0.5)
@



The graph of the distribution function  shows that it is
strictly increasing. For example, the exact value for the quantile $x_{0.025}$ can be computed
by

<< BCL2-3, echo = TRUE >>=
qnorm(0.025,8.6,0.5)
@



That is, the quantile $x_{0.025}$ = 0.92. Hence, it holds that the probability of
observing values less than $0.92$ equals $0.025$, that is $P (X \leq 0.92) = 0.025$,
as can be verified by ` pnorm(0.92, 8.6, 0.5)`.

 When $X$ is distributed as
$N (8.6, 0.5 )$, then the population mean is $8.6$ and the population standard
deviation $0.5$. To verify this we draw a random sample of size $1000$ from this
population by

<<  BCL2-4, echo = TRUE >>=
x <- rnorm(1000,8.6,0.5)
@


The estimates

<< BCL2-5, echo = TRUE >>=
mean(x)
#and
sd(x)
@

are close to their population values $\mu = 8.6$ and $\sigma = 0.5$.

\subsubsection*{Exercise: Normal Model for a gene}

Suppose that the distribution of the expression values for a
gene is distributed according to $N(1.6, 0.42)$.

1. Compute the probability that the expression values are less
than 1.2.
2. What is the probability that the expression values are between 1.2
and 2.0?
3. What is the probability that the expression values are between 0.8
and 2.4?
4.  Compute the exact values for the quantiles $x_{0.025}$ and $x_{0.975}$.

5. Use \Rfunction{rnorm} to draw a sample of size 1000 from the population and
compare the sample mean and standard deviation to that of the population.




\subsection{Conducting a t-test}

Suppose that gene expression data from two groups of patients (experimental conditions)
are available and that the hypothesis is about the difference
between the population means $\mu_1$ and $\mu_2$ . In particular, $H_0 : \mu_1 = \mu_2$
is to be tested against $H_1 : \mu_1 \neq \mu_2$. These hypotheses can also be formulated
as $H_0 : \mu_1 - \mu_2 = 0$ and $H_1 : \mu_1 - \mu_2 \neq 0$. Suppose that gene expression
data from the first group are given by $\{x_1 , \dotsc, x_n \}$ and that of the second by
$\{y_1 , \dotsc, y_m \}$. Let $\bar x$ be the mean of the first and $\bar y$ that of the second,
$s_1$ the variance of the first and  $s_2$ that of the second. Then the $t$-statistic can
be formulated as

$$
t = \frac{ \bar x - \bar y -(  \mu_1 - \mu_2 ) }{ s_1/\sqrt{n} + s_2/\sqrt{m}}
$$


The decision procedure with respect to the null-hypothesis is completely analogous
to the permutation test. However, the sampling distribution is
\textbf{found analytically based on assumptions on the data and not by permutations}.

Note that the $t$--value is large if the difference between
$x$ and $y$ is large, the standard deviations $s_1$ and $s_2$ are small, and the sample
sizes are large. This means for example that higher sample sizes allow you to detect
more subtle mean differences. The $t$--test  with the assumptions of unequal variances
in the groups is also known as the Welch two-sample $t$--test and is routinely performed.

If $s_1 = s_2$ the variance estimator of the $t$--test and the calculation of the degrees
of freedom of the $t$--distribution changes slightly. This test is available by specifying
\Rfunction{var.equal = TRUE} when calling the function \Rfunction{t.test}.

\subsubsection*{Example: Comparing BCL2 between BCR/ABL and NEG  }
The  gene BCL2 plays an important role with respect to discriminating
BCR/ABL from NEG patients. The null hypothesis of equal means can be tested by the function
\Rfunction{t.test} and the appropriate factor and specification to separate the groups.
(\Rfunction{var.equal=FALSE}  by default).

<<tTestBCL2,  echo = TRUE>>=
t.test(expALL[1152,] ~ groupsALL)
### alternative call
t.test(expALL[1152, groupsALL == "BCR/ABL"],
       expALL[1152, groupsALL == "NEG"] )
@
The $t$-value is quite large, indicating that the two means $x$ and $y$ differ largely
from zero relative to the corresponding standard error . Since the $p$--value is extremely small,
the conclusion is to reject the null--hypothesis of equal means. The data provide
strong evidence that the population means do differ.



\section{Wilcoxon rank test}
In case the data are normally distributed with equal variance, the $t$--test is
an optimal test for testing $H_0 : \mu_1 = \mu_2$  against $H_1 : \mu_1 \neq \mu_2$.
 If, however, the data are not normally distributed due to skewness or
otherwise heavy tails, then this optimality does not hold anymore and there
is no guarantee that the significance level of the test equals the intended
level $\alpha$. Usually, one will  loose power if the normality assumption
is validated, i.e. the  $\alpha$ will be inflated.

For this reason rank type of tests are developed for which on beforehand
no specific distributional assumptions need to be made.
In the example below we shall concentrate on the two-sample Wilcoxon test.


To broaden our view we switch from hypotheses about means to those about distributions.
An alternative hypothesis may then be formulated as that the distribution of a first
group lays to the
left of a second.


To set the scene let the gene expression values of the first
group ($x_1$ to $x_m$) have distribution $F$ and
those of the second group ($y_1$ to $y_n$) distribution $G$.
The null hypothesis is that both distributions are equal
($H_0 : F = G$) and the alternative that they are not.

For example that the $x$'s are smaller (or larger) than the $y$'s.
By the two-sample Wilcoxon test the data $x_1 , \dotsc , x_m$ ,
$y_1 , \dotsc , y_n$ are ranked and the rank numbers of the $x$'s are
summed to form the statistic $W$ after a certain correction.

The idea is that if the ranks of $x$'s are smaller than those of the $y$'s, then the
sum is small. The distribution of the sum of ranks is known so that a $p$--value
can be computed on the basis of which the null hypothesis is rejected if it is
smaller than the significance level  $\alpha$.




\subsubsection*{Example: BCL2 gene}
The null hypothesis that the expression values for gene
BCL2  are equally distributed for the ALL patients and the AML
patients can be tested by the built--in--function \Rfunction{wilcox.test}, as follows.



<< wilcoxTestBCL2,  echo = TRUE>>=
wilcox.test(expALL[1152,] ~ groupsALL)
@

Since the $p$--value is much smaller than $\alpha = 0.05$, the conclusion is to reject
the null--hypothesis of equal distributions.


\subsubsection*{A permutation test for BCL2}

We can of course also test the BCL2 gene for differential expression using
a permutation test:

<<  permTestBCL2 >>=
pTest <- permTestTwoGroups(expALL[1152, groupsALL == "BCR/ABL"],
       expALL[1152, groupsALL == "NEG"], permutations = 1e4 )

pTest$pval

pTest$samplingDistPlot
@

The result is similar to the other tests performed.

\subsection{Where permutation test do not apply}

Permutation tests are helpful and their widespread use has been made possible by
the computer power we have at our disposal today. However, they are not a panacea.

We have seen that in the two groups case,  permutation testing is straightforward.
The same is true for the testing of the dependence between two variables for example, while
other situations are not easily covered by a permutation test, such testing a single
sample mean. Also, we cannot perform a permutation test of the mean difference if the variance
in the two groups differ, since then we cannot pool the data.



\subsection{Caveat: Wilcoxon test vs. t--test}

The t--test requires normally distributed data in order to be valid. In practice,
this leads to many people prefer Wilcoxon tests over the t--test. However,
the problem with the Wilcxocon test is that it implicitly assumes \textbf{equal variances}
in the two groups, since it only  tests for shifts in location.
So if the variances are not equal, it can give misleading results.

It actually often leads to overly low p--values. Below is a little simulation study
showing this effect. Two groups of 10 normally distributed values are simulated,
one with a standard deviation of 1 and another with a standard deviation of 15.
There is no difference between the groups (both have mean 0), although
the standard deviations are very different, so we expect a
proportion of 5\%  significant p--values at an $\alpha$--level of 5\%.

<< WilcoxSim>>=


x <- rnorm(10)
qqnorm(x)
qqline(x)


y <- rnorm(10)
wilcox.test(x,y)



wc <- function(){
  x <- rnorm(10)
  y <- rnorm(10, sd = 15)
  tt <- wilcox.test(x,y)
  tt$p.value
}

pVals_WC <- replicate(1000, expr = wc())

hist(pVals_WC, col = "tan3", main = "Wilcoxon P--values")
prop.table(table(pVals_WC < 0.05))


set.seed(999)

ttest <- function(){
  x <- rnorm(10)
  y <- rnorm(10, sd = 15)
  tt <- t.test(x,y)
  tt$p.value
}

pValsT <- replicate(1000, expr = ttest())

hist(pValsT, col = "coral3", main = "t-test P--values")
prop.table(table(pValsT < 0.05))

@


the t--test pvalues are correct (uniformly distributed) and
the alpha level is kept, while the Wilcoxon test is too optimistic and
has an actual level of near 10\% at a nominal level
of 5\%. So there are too many false positives for the Wilcoxon test.




\section{Chi--squared Test and the fisher test for contingency tables}

The test above treat continuous data, We now turn to tests for categorical data.
Typically, categorical data is represented in the form of contingency
tables, where one categorization is represented by the rows and the other
by the columns. A $\chi^2$ test then tests for a  for independence of
rows and columns in an $r \times c$ contingency table. It will tell us, whether
the row classifications are independent of the column classifications in a table like
this:



\begin{center}
\includegraphics[width=6cm,height=3.7cm]{cont-table}
\end{center}

The actual number observations in each cell of the table can be
compared to the expected number of observations under the assumption
of independent row and column classifications and is given by

\begin{center}
\includegraphics[height = 1.5cm, width = 5cm]{observed-cells}
\end{center}

and a $\chi^2$ statistic can be computed as above:
\begin{gather*}
\chi^2 = \sum_{i=1}^{r} \sum_{j=1}^{c} {(O_{ij} - E_{ij})^2 \over E_{ij}}
\end{gather*}
it has $(r - 1)(c - 1)$  degrees of freedom.

\subsection{Fishers tea tasting experiment and genetics}

One of the most famous examples of hypothesis testing was performed by RA Fisher on a lady
that claimed could tell if milk was added before or after the tea was poured.
Fisher gave the lady four pairs of cups of tea:
one with milk poured first, the other after. The order was randomized.
Say the lady picked 3 out 4 correctly, do we believe she has a special ability? Tests
for discrete data help to answer this question by quantifying what happens by chance.

The basic question we ask is: if the lady is just guessing, what are the chances that she
gets 3 or more correct?  If we assume the lady is just guessing randomly, we can think of
this particular examples as picking 4 balls out of an urn with 4 green (correct answer) and
4 red (incorrect answer) balls.

Under the null hypothesis that the lady is just guessing each ball has the same chance of
being picked. We can then use combinatorics to figure out the probability. The probability
of picking 3 is
${4 \choose 3} {4 \choose 1} / {8 \choose 4} = 16/70$.
The probability of picking all correct is
${4 \choose 4} {4 \choose 0}/{8 \choose 4}= 1/70$. Thus the chance of observing a 3 or
something more extreme, under the null hypothesis, is 0.24. This is the $p$--value.
This is called Fisher's exact test and it uses the hyper geometric distribution. It is not
appropriate for most the tests applied in genetics but the idea is similar.

For example, imagine we have 250 individuals, some of them have a given disease
others don't. We observe that a 20\% of the individuals that are homozygous for the minor allele have the disease compared to 10\% of the rest. Would we see this again if we picked another 250 individuals?

Here is an example dataset
<<Gentetic association test>>=
disease=c(rep("no",180),rep("yes",20),rep("no",40),rep("yes",10))
genotype=c(rep("AA",200),rep("aa",50))
tab=table(genotype,disease)
tab
@

The null-hypothesis is that the 200 and 50 individuals in each group were assigned disease with the same probability. If this is the case then the probability of disease is
<<Gentetic association test 2>>=
p <- mean(disease == "yes")
p
@
The expected table is therefore
<<Gentetic association test 3>>=
rbind(c(1-p,p)*sum(genotype=="aa"),c(1-p,p)*sum(genotype=="AA"))
@

We can compute an $\chi^2$ statistic of seeing a deviation for the expected table
as big as this one.
The p-value for this table is
<<Gentetic association test 4>>=

chisq.test(tab)$p.value
@

Note that there is not a one to one relationship between the odds ratio
($\frac{n_{11}}{n_{12}} /  \frac{n_{21}}{n_{22}} $ )
and the p-value. If we increase the numbers but keep the difference
in proportions the same, the $p$--value is reduced substantially:

<<Gentetic association test 5>>=
tab=tab*10
chisq.test(tab)$p.value
@


\subsection{Simple gene set enrichment analysis}
Suppose that the number of  onco--type of genes in Chromosome 1
is $n_{11} = 100$ out of a total of $n_{12} = 200$ genes
and the number of onco--genes in the rest of the genome
is $n_{21} = 300$ out of a total of $n_{22} = 6000$ genes as summarized in the
table.

\begin{table}[h!t]
\centering
\begin{tabular}{|r|rr|c|}
  \hline
 & onco--genes & non--onco-genes & row--sums \\
  \hline
Chromosome 1 & 100 & 200 & 300\\
Rest of Genome & 3000 & 6000 & 9000 \\
   \hline
 column--sums & 3100 & 6200 & 9300 \\
   \hline
\end{tabular}
\end{table}

The $\chi^2$ test will now tell us, whether there is a significantly higher
or lower proportion of onco--genes in chromosome 1 than in the  rest of the genome.
Chromosome 1 serves as our gene set here.
In biology, over--representation is often called ``enrichment'' and an under--representation
is called ``depletion'' and hence the  $\chi^2$ test for this table can be viewed
as test of an onco--gene enrichment/depletion in the gene set chromosome 1:

<<chi2-enrich>>=
dat1 <- matrix(c(100,200,3000,6000),2,byrow=TRUE)
## Chi2 test
chisq.test(dat1)
@
An alternative to the $\chi^2$ test for 2 $\times$ 2 tables is the
 Fisher--test. It tests whether
the odds ratio $\frac{n_{11}}{n_{12}} /  \frac{n_{21}}{n_{22}} $ is significantly
different from 1, which would again indicate a ``depletion'' (if OR $< 1$) or enrichment
(if OR  $>1$) of oncogenes in chromosome 1 in this case.
As we can see the odds ratio is 1, i.e. there is neither enrichment nor depletion.

<<chi2-enrich-fisher>>=
dat1 <- matrix(c(100,200,3000,6000),2,byrow=TRUE)

## Fisher test
fisher.test(dat1)

@


As yet another alternative, we can compare the proportion of onco--genes in chromosome
1 to the the proportion of  onco--genes in the rest of the genome. As we can see,
the test of proportions also returns a $p$--value of 1.


<<chi2-enrich-prop>>=
dat1 <- matrix(c(100,200,3000,6000),2,byrow=TRUE)
## Comparison of proportions of oncogenes in the two subsets
dat1.prop <- matrix(c(dat1[1,1], dat1[2,1],dat1[1,1] + dat1[1,2],
               dat1[2,1] + dat1[2,2]), 2,2,byrow=TRUE)
prop.test(dat1.prop[1,] ,dat1.prop[2,])

@



Let's look at some additional data: the table below shows an example of an
under--representation of onco--genes in Chromosome 1

\begin{table}[h!t]
\centering
\begin{tabular}{|r|rr|c|}
  \hline
 & onco--genes & non--onco-genes & row--sums \\
  \hline
Chromosome 1 & 50 & 250 & 300 \\
Rest of  Genome & 3000 & 6000 & 9000 \\
   \hline
 column--sums & 3050 & 6250 & 9300 \\
   \hline
\end{tabular}
\end{table}

%

$\chi^2$ test
<<chi2-enrich-2>>=
dat2 <- matrix(c(50,250,3000,6000),2,byrow=TRUE)
## Chi2 test
chisq.test(dat2)
## Fisher test
fisher.test(dat2)
## Comparison of proportions of oncogenes in the two subsets
dat2.prop <- matrix(c(dat2[1,1], dat2[2,1],dat2[1,1] + dat2[1,2],
               dat2[2,1] + dat2[2,2]), 2,2,byrow=TRUE)
prop.test(dat2.prop[1,] ,dat2.prop[2,])
@
%

Both the $\chi^2$ test as well as the Fisher test are significant.
The odds ratio is $\frac{50}{250} /  \frac{3000}{6000}  = 0.4$
showing a depletion (OR $< 1$) of oncogenes in chromosome 1.
The test of proportions also gives a significant $p$--value.

The odds ratio is very often used as a measure of association in
2 $\times$ 2 tables since the absolute value of the natural logarithm
of the odds ratio (often called lod--score) given by

\begin{gather*}
\text{lod} = \abs{\ln\left(\frac{n_{11}}{n_{12}} /  \frac{n_{21}}{n_{22}}\right)}
\end{gather*}

is only dependent on the cell contents
of table, i.e. shuffling rows or columns does not change it.

%ex.df <- data.frame( significant = c(100,300), non_significant = c(2000,6000),
%row.names = c("Chromosome_1", "genome") )
%print(xtable(ex.df  ), type = "latex")


\subsubsection*{Exercise: Rocky mountain spotted fever}

In 747 cases of "Rocky Mountain spotted fever" from the western United States,
210 patients died.
Out of 661 cases from the eastern United States,
122 died. Is the difference statistically significant?
Use a prop--test as well as a Fisher--test.

\section{Multiple testing}

When performing a large number of tests, the Type I error is inflated:
Let's assume we perform  $m$ tests with Type I error rate $\alpha$ (reject $H_0$
although  $H_0$ is true) of 5\%. Then the probability of  \textbf{no false rejection}
if the tests are independent is:

\begin{gather}
\underbrace{0.95\cdot{0.95}\cdot \dotso \cdot {0.95}}_\text{m--times} \ggg  0.95
\end{gather}

Thus, the larger the number of tests performed, the higher the probability
of a false rejection (= Type I error, false positive)

However, this problems is often put aside and hypothesis testing/significance
analysis is commonly used in a too simple way. Correcting for multiple testing
helps to avoid false positives or discoveries. There are two key components
of a multiple testing procedure:
\begin{itemize}
  \item Error measure
  \item Correction procedure / estimation algorithm
\end{itemize}


\subsection{Types of errors and error rates}

Suppose you are testing a hypothesis that a parameter $\beta$ equals zero versus
the alternative that it does not equal zero. Let us assume that there are
$m_0$ number of tests that correspond to a true null hypothesis out of $m$ total
tests and that we reject $R$ null hypotheses in total. These are the possible outcomes:


\begin{table}[h!t]
\centering
\begin{tabular}{|r|rr|c|}
  \hline
 & $\beta=0$  & $\beta\neq0$ & Hypotheses  \\
  \hline
Claim $\beta=0$  & True Positive  & False Negative & $m-R$\\
Claim $\beta\neq 0$  & False Positive & True Negative  &  $R$ \\
   \hline
Claims  & $m_0$  & $m-m_0$ & $m$\\
   \hline
\end{tabular}
\end{table}

\begin{itemize}
  \item Type I error or false positive  --- Say that the parameter
        does not equal zero when it does
  \item Type II error or false negative  --- Say that the parameter
  equals zero when it doesn't
\end{itemize}

Just like ordinary significance testing tries to control the false positive
rate, there are other types of rates commonly used in multiple testing procedures:

\begin{itemize}
  \item \textbf{False positive rate} - The rate at which false results ($\beta = 0$) are
called significant: $E\left[\frac{FP}{m_0}\right]$

  \item \textbf{Family wise error rate (FWER)} - The probability of at least
one false positive ${\rm Pr}(FP \geq 1)$

  \item \textbf{False discovery rate (FDR)} - The rate at
  which claims of significance are false  $E\left[\frac{FP}{FP + TP}\right]$
\end{itemize}


If $p$--values are correctly calculated calling all $p < \alpha$ significant
will control the false positive rate at level $\alpha$ on average.



\subsection{Control of  error rates}

Suppose that you perform 10,000 tests and $\beta = 0$ for all of them.
and  you call all $P < 0.05$ significant.
Then expected number of false positives is: $10,000 \times 0.05 = 500$
false positives. How do we avoid so many false positives?

\subsubsection{Controlling family-wise error rate (FWER)}

The Bonferroni correction is the oldest multiple testing correction.

\underline{Basic algorithm}
\begin{itemize}
    \item Suppose you do $m$ tests
  \item You want to control FWER at level $\alpha$ so $Pr(FP \geq 1) < \alpha$
  \item Calculate $p$--values normally
  \item Set $\alpha_{fwer} = \alpha/m$
  \item Call all $p$-values less than $\alpha_{fwer}$ significant
\end{itemize}

The bonferroni correction is easy to calculate but very conservative.


\subsubsection{Controlling false discovery rate (FDR)}

This is the most popular correction when performing lots of tests as in in genomics.
It is often termed the Benjamini Hochberg procedure and controls
the FDR.

\underline{Basic algorithm }

\begin{itemize}
  \item Suppose you do $m$ tests
  \item You want to control FDR at level $\alpha$ so
  $E\left[\frac{FP}{TP + FP}\right] < \alpha$
  \item Calculate $p$--values normally
  \item Order the $p$--values from smallest to largest $p_{(1)},...,p_{(m)}$
  \item Call any $p_{(i)} \leq \alpha \times \frac{i}{m}$ significant
\end{itemize}

The FDR control procedure is still pretty easy to calculate and
less conservative (possibly much less) than  controlling the FWER.
On the contrary, it  allows for more false positives and
may behave strangely under dependence.


\subsubsection{Adjusted $p$--values}

The  approach indicated above is to adjust the threshold $\alpha$,  a different
approach is to calculate ``adjusted $p$--values". They are not $p$--values anymore but
they can be used directly without adjusting $\alpha$.

Example

\begin{itemize}
  \item Suppose p-values are $p_1,\ldots,p_m$
  \item You could adjust them by taking $p_i^{fwer} = \max\{m \times p_i,1\}$ for each $p$--value.
  \item Then if you call all $p_i^{fwer} < \alpha$ significant you will control the FWER.
\end{itemize}

\subsection{Diagnostic plots for multiple testing procedures}
The code below simulates  $m = 200$ $p$--values from the mixture model

\begin{gather*}
0.75\cdot N(0,1) + 0.25 \cdot N(2,1)
\end{gather*}

,i.e. $m_0 = 150$ here and the null distribution is
the standard normal distribution. \textbf{It is an important fact hat for a
continuous null distributions the corresponding $p$--values are uniform}.

<<simulate z scores>>=

sd.true =1
eta0.true = 0.75

get.random.zscore = function(m=200)
{

  m0 = m*eta0.true
  m1 = m-m0
  z = c(  rnorm(m0, mean=0, sd=sd.true),
       rnorm(m1, mean=2, sd=1))
  #z = sign(rnorm(length(z)))*z

  return(z)
}

set.seed(555)
z <- get.random.zscore(200)
pv = 1- pnorm(z, sd = sd.true)

@

\subsubsection{Schweder and Spj\o{}tvoll plot}


If a test statistic does not correspond to a true null hypothesis, the corresponding
$p$--value will be very small.
For large $p$--values which likely  will
correspond to true null hypotheses it
then holds that
\begin{gather*}
E[M(p)] = m_0(1-p)  \, .
\end{gather*}

Large (probably non--null) $p$--values will thus be close to a straight
line with slope $m_0$. Accordingly, small
$p$--values (probably null) will deviate from that line.

Schweder and Spj\o{}tvoll (Biometrika, 1982) suggested to use these
facts for a diagnostic plot
of the observed $p$-values which permits estimation of the fraction of true null
hypotheses. For a series of hypothesis tests $H_1, \ldots, H_m$ with $p$-values
$p_i$, they suggested plotting
%
\begin{equation*}
  \left( 1-p_i, M(p_i) \right) \mbox{ for } i \in 1, \ldots, m,
\end{equation*}
%
where $M(p)$ is the number of $p$--values greater than $p$. An application of
this diagnostic plot to our simulated $p$--values can be seen in the figure.

When  the first $m_0$ null hypotheses are true and
the other $m-m_0$ are false, the cumulative distribution function of $(1-p_1,
\ldots, 1-p_{m_0})$ is  expected to be close to the line $F_0(t)=t$. The
cumulative distribution function of $(1-p_{m_0+1}, \ldots, 1-p_{m})$, on the
other hand, is expected to be close to a function $F_1(t)$ which stays below
$F_0$ but shows a steep increase towards 1 as $t$ approaches $1$.
In practice, we do not know which of the null hypotheses are true, so we can
only observe a mixture whose cumulative distribution function is expected to be
close to
%
\begin{equation*}
  F(t) = \frac{m_0}{m} F_0(t) + \frac{m-m_0}{m} F_1(t).
\end{equation*}
%
In our simulated data  $F_0 = 1-N(0,1)$ and $F_1 = 1-N(2,1)$. By looking
at the figure, the points start to deviate at 150 from the straight line,
as expected from the simulation model.

<<SchwSpot_plot >>=
(ggplot2::qplot(sort(1-pv), 1:200, xlab = "1 - p-values", ylab = "No. of hypotheses",
       main = "Schweder and Spotvoll plot")
 + geom_abline(intercept = 0, slope = 200*eta0.true, aes(color = "coral3")))
@

\subsubsection{Histogram of $p$--values}
As already mentioned, the $p$--values follow a uniform distribution on the unit interval
[0,1] if they are computed using a continuous null distribution. Significant $p$--values
thus become visible as an enrichment of $p$--values near zero in the histogram.
A histogram of $p$--values should always be plotted in order to check whether
they have been computed correctly.

<<pvalue_histogram >>=
ggplot2::qplot(x = pv, xlab = "p-values", main = "Histogram of p-values, correct null
        distribution",
       fill = I("navyblue"))
@
We see that our $p$--values are uniformly distributed under the null hypotheses.
Computing the $p$--values assuming a $N(0,2)$ null distribution changes the picture.

<<pvalue_histogram_wrong_null>>=
ggplot2::qplot(x = pnorm(z, sd = 2) , xlab = "p-values", main = "Histogram of p-values,
      variance of null distribution too high",
       fill = I("coral3"))
@

If the assumed variance of the null distribution is too high, we often see
hill--shaped $p$--value histogram. If the variance is too low, we get a U--shaped
histogram, with peaks at both ends.

<<pvalue_histogram_wrong_null_2 >>=
ggplot2::qplot(x = pnorm(z, sd = 0.5) , xlab = "p-values", main = "Histogram of p-values,
      variance of null distribution too low",
       fill = I("chartreuse4"))
@

\subsection{Computing multiple testing adjustments}
The most commonly used multiple testing adjustments can be computed using
the function \Rfunction{padjust}. To compute the Benjamini Hochberg
adjusted $p$--values simply specify \Rcode{ method = "BH"}. For FEWR
control we choose  \Rcode{ method = "bonferroni"}.

<<p-value_adjustments>>=
alpha = 0.05
pv.BH <- p.adjust(pv, method = "BH")
table(pv.BH < 0.05)
(ggplot2::qplot(rank(pv), pv, xlab = "p-value rank", ylab="p-values"
       ,main = "Visualization of the BH procedure")
+ geom_abline(intercept = 0, slope = alpha/200, aes(color = "coral3"))
+ ylim(c(0, 0.2) ))
pv.FWER <- p.adjust(pv, method = "bonferroni")
table(pv.FWER < 0.05)
@

The figure illustrates of the Benjamini-Hochberg multiple testing
adjustment. The black line shows the
$p$-values ($y$-axis) versus their rank ($x$-axis), starting with
the smallest $p$-value from the left, then the second smallest, and
so on.
The red line is a straight line with slope $\alpha/m$, where
$m$ is the number of tests, and
$\alpha$ is a target false discovery rate.  FDR
is controlled at the value $\alpha$ if the genes are selected
that lie to the left of the rightmost intersection between the red and black
lines: here, this results in \Sexpr{sum(pv.BH < 0.05)} significant $p$--values.
Thus, the procedure is relatively conservative since we actually have simulated
25 non--null $p$--values. The Bonferroni correction is clearly not suitable here as
we only get 3 significant $p$--values.

\subsection{Modifying the BH procedure to gain power and the q--value}

The \CRANpkg{mutoss} package provides many more multiple testing adjustments. In
addition, it also has  the function \Rfunction{ABH\_ pi0\_ est}  that estimates
the proportion $\pi_0$ of the null model (in our case 75\%) for us based
on the Schweder and Spj\o{}tvoll plot. Here,
it estimates $\pi_0$ as \Sexpr{mutoss::ABH_pi0_est(pv)}  which is quite far
away from the true value. However, with only 200 test statistics, it also
difficult to estimate $\pi_0$ reliably.

<<mutoss oracleBH>>=
 ABH_pi0_est(pv)
pv.OracleBH <- oracleBH(pValue=pv, alpha=alpha, pi0=0.75)
@

The BH procedure implicitly assumes $\pi_0 = 1$, i.e. that there are no
non--null p-values in its original form. Thus,
we can gain power, by plugging in an estimate of $\pi_0$. Indeed, we gain
\Sexpr{sum(pv.OracleBH$rejected) - sum(pv.BH < alpha)}
more rejections.


The q--value is an FDR estimation procedure roughly defined as a BH procedure
combined with a $pi_0$ estimate --- pretty much like the oracle BH procedure above and is
very popular in  genomics. It tries to estimate $\pi_0$ from the $p$--value histogram. Note that it actually tries to estimate the FDR for a test statistic rather than just providing
a control of the FDR as the BH procedure.

<<mutoss Qvalue>>=
pv.Qvals <- Qvalue(pv)
table(pv.Qvals$qValues < 0.05)
@
Although the number of $p$--values is low, the q--value procedure estimates $pi_0$
correctly and provides the same number of rejected hypotheses as oracle BH procedure.


\section{Regularized $t$--tests for small $n$, large $p$ problems}

In microarray analyses, on usually uses a variant of a (regularized) $t$--statistic
that is suitable for high-dimensional data and large-scale multiple testing such
as the one implemented in the Bioconductor package \Biocpkg{limma}.

The basic statistic used for significance analysis in \Biocpkg{limma} is the moderated
$t$--statistic, which is computed for each gene separately.
It has the same interpretation as an ordinary $t$--statistic except that
the standard errors have been moderated across genes, i.e., shrunk toward a common value,
using a simple Bayesian model. This has the effect of borrowing information from the
ensemble of genes to aid with inference about each individual gene.
Moderated t-statistics lead to p- values in the same way that ordinary t-statistics do
except that the degrees of freedom are increased, reflecting the greater
reliability associated with the smoothed standard errors.

\subsection{Some details of the \Biocpkg{limma} method}

The empirical Bayes method in  \Biocpkg{limma} assumes an inverse Chi-square prior
for the variance $\sigma^2$ with mean $s_0^2$ and degrees of freedom $f_0$. These
parameters are estimated from data and not set beforehand, hence this is an
"empirical" Bayesian method.

The posterior values for the residual variances are given by
\begin{gather*}
s_j^2 = \frac{f_0s_0^2 + f\sigma^2}{f_0 + f}
\end{gather*}
Where $f$ denotes the degrees of freedom for a gene. For two group comparison
$f =  n_1 + n_2 - 2$, where $n_1$  and  $n_2$ are the number of samples in each of the
groups.

\subsection{Shrinkage estimation}

The most important aspect of the \Biocpkg{limma} approach is that \Biocpkg{limma}
performs a SHRINKAGE of the variances towards a target, which is given by $s_0^2$, the
prior mean variance and the shrinkage intensity is $\frac{f_0}{f_0 + f}$.

A $t$--test with $f_0 + f$ degrees of freedom using the shrunken variances
is  then computed to assess differential expression. There are many other
ways to perform shrinkage, which will commonly improve the estimation of the
variance by sharing information across genes. A wide  selection of these
statistics is implemented in the package \CRANpkg{st} We use the \Rfunction{modt.stat}
from the package  to compute \Biocpkg{limma}s
moderated $t$--statistic for BCL2 in the ALL data:


<<modTestBCL2,  echo = TRUE>>=

modt.stat(t(expALL), groupsALL)[1152]
### p-value using the normal distribution
2 - 2*pnorm(modt.stat(t(expALL), groupsALL)[1152])
### slightly higher t-value than the ordinary t-test
t.test(expALL[1152,] ~ groupsALL, var.equal=FALSE)$statistic
@

Since information about the overall variance of the genes in the data set is used
to compute the variance for CCND3, the whole data set has to be provided in
order to compute the moderated $t$--statistic.

\subsubsection*{Exercise: Group comparison for gene GYPC}
The gene GYPC plays an important role in regulating the mechanical
stability of red cells. I can be found
in line \Sexpr{grep("GYPC", anno_fusALL$SYMBOL)} of the expALL data set. (Try
\Rfunction{grep("GYPC", anno\textunderscore fusALL\$SYMBOL))}.

Test for the equality of the
means by an appropriate $t$--test. Is the experimental effect very strong?
Also, try testing the hypothesis  using a moderated $t$--test and a wilcoxon
test.



\subsection{Multiple testing applied to the ALL data set}
We illustrate some multiple testing approaches with the expALL data set. For
this we use the shrinkage t statistic from the  \CRANpkg{st} package which
implements an analytical rather than a Bayesian shrinkage approach. First,
we compute the $t$--statistics and the associated tow--sided $p$--values using a standard
normal distribution as the null model. The resulting $p$--value histogram looks
good.

<<shrinkage_t_ALL>>=
sts <- shrinkt.stat(t(expALL), groupsALL)
### p-value using the normal distribution
pval.st <- 2 - 2*pnorm(abs(sts))
hist(pval.st, col = "lavender", main = "Histogram of p-values for
     shrinkage t statistics of the ALL data")
@

We can compute a standard BH adjustment to obtain the number of differentially
expressed genes at an FDR of 5\%.

<<shrinkage t multiple testing>>=
pv.st.BH <- p.adjust(pval.st, method = "BH")
table(pv.st.BH< 0.05)

@


\subsubsection*{Exercise: Multiple testing for the ALL data}
Try other multiple testing procedures like q--values on the $p$--values
obtained from the shrinkage t statistics. Can you gain power? Produce a $p$--value
histogram where significant statistics are indicated by color-fill.


\section{Indepdendent filtering of hypotheses}

We have seen in the previous sections that multiple testing approaches,
with thousands of tests, are often used in analyses of genome-scale data.
For instance, we have seen that in the analyses of
differential gene expression based on RNA-Seq or microarray data, a
common approach is to apply a statistical test, one by one, to each of
thousands of genes, with the aim of identifying those genes that have
evidence for a statistical association of their expression
measurements with the experimental covariate(s) of interest.

Another instance is differential binding detection from ChIP-Seq data.  The
idea of \emph{independent filtering} is to filter out those tests from
the procedure that have no, or little chance of showing significant
evidence, without even looking at their test statistic. Typically,
this results in increased detection power at the same experiment-wide
type I error, as measured in terms of the false discovery rate.  A
good choice for a filtering criterion is one that
\begin{enumerate}
  \item\label{it:indp} is statistically independent from the test statistic
    under the null hypothesis,
  \item\label{it:corr} is correlated with the test statistic under the
    alternative, and
  \item\label{it:joint} does not notably change the dependence
    structure --if there is any-- of the joint test statistics
    (including those corresponding to true nulls and to true
    alternatives).
\end{enumerate}
The benefit from filtering relies on property~\ref{it:corr}.
The statistical validity of filtering relies on
properties \ref{it:indp} and \ref{it:joint}.  For many practically useful combinations of
filter criteria with test statistics, property~\ref{it:indp} is easy to prove.
Property~\ref{it:joint} is more complicated, but rarely
presents a problem in practice: if, for the multiple testing procedure that is being used,
the correlation structure of the tests was acceptable without filtering, the filtering should
not change that.


%-----------------------------------------------------------
\subsection{Example data set}
%-----------------------------------------------------------
For illustration, let us use the \Robject{pasillaGenes} dataset from the
Bioconductor package \Biocpkg{pasilla}; this is an RNA-Seq dataset
from which we extract gene-level read counts for two replicate samples
the were measured for each of two biological conditions: normally
growing cells and cells treated with dsRNA against the \emph{Pasilla}
mRNA, which led to RNAi interference (RNAi) mediated knockdown of the
Pasilla gene product.
%
<<libraries,results='hide'>>=
data("pasillaGenes")
@
%
We perform a standard analysis with \Rpackage{DESeq} to look for genes
that are differentially expressed between the normal and
Pasilla-knockdown conditions, indicated by the factor variable
\Robject{condition}. In the generalized linear model (GLM) analysis,
we adjust for an additional experimental covariate \Robject{type},
which is however not of interest for the differential expression. For
more details, please see the vignette of the \Rpackage{DESeq} package.

<<DESeq2,cache=TRUE,results='hide', warning = FALSE>>=
cds  <- estimateSizeFactors( pasillaGenes )
cds  <- estimateDispersions( cds )
fit1 <- fitNbinomGLMs( cds, count ~ type + condition )
fit0 <- fitNbinomGLMs( cds, count ~ type  )
@

<<DESeq3,cache=TRUE>>=
res <- data.frame(
filterstat <- rowMeans(counts(cds)),
pvalue    <- nbinomGLMTest( fit1, fit0 ),
row.names <-featureNames(cds) )
@
%
The details of the anove analysis are not important for explaing the benefits of
filtering, the essential output is contained in the columns of the
dataframe \Robject{res}:
\begin{itemize}
  \item \texttt{filterstat}: the filter statistic, here the average
    number of counts per gene across all samples, irrespective of
    sample annoation,
  \item \texttt{pvalue}: the test $p$-values,
\end{itemize}
Each row of the dataframe corresponds to one gene:
<<headres>>=
dim(res)
head(res)
@

%--------------------------------------------------
\subsection{Qualitative assessment of the filter statistic}\label{sec:qual}
%--------------------------------------------------
<<pass,echo=FALSE,cache=FALSE>>=
theta = 0.4
pass = with(res, filterstat > quantile(filterstat, theta))
@
%
First, consider the Figure below, which shows that
among the approximately \Sexpr{100*theta}\% of genes with lowest overall counts, \Robject{filterstat},
there are essentially none that achieved an (unadjusted) $p$-value less than
\Sexpr{signif(quantile(res$pvalue[!pass], 0.0001, na.rm=TRUE), 1)}
(this corresponds to about \Sexpr{signif(-log10(quantile(res$pvalue[!pass], 0.0001, na.rm=TRUE)), 2)} on the $-\log_{10}$-scale).
%
<<figscatterindepfilt, fig.show = 'hide'>>=
with(res,
  plot(rank(filterstat)/length(filterstat), -log10(pvalue), pch=16, cex=0.45))
@
<<figecdffilt, fig.show = 'hide'>>=
trsf = function(n) log10(n+1)
plot(ecdf(trsf(res$filterstat)), xlab=body(trsf), main="")
@
\begin{figure}[ht]
\centering
\includegraphics[width=.49\textwidth]{figure/figscatterindepfilt-1}
\includegraphics[width=.49\textwidth]{figure/figecdffilt-1}
\caption{Left: scatterplot of the rank (scaled to $[0,1]$) of the
  filter criterion \Robject{filterstat} ($x$-axis) versus the negative
  logarithm of the test \Robject{pvalue} ($y$-axis). Right: the
  empirical cumulative distribution function (ECDF) shows the
  relationships between the values of \Robject{filterstat} and its
  quantiles.}
\label{figscatterindepfilt}
\end{figure}
%
This means that by dropping the 40\% genes with lowest \Robject{filterstat},
we do not loose anything substantial from our subsequent
results.

For comparison, suppose you had chosen a less useful filter statistic,
say, the gene identifiers interpreted as a decimal number. The
analogous scatterplot to that of Figure~\ref{figscatterindepfilt} is
shown in Figure~\ref{figbadfilter}.
%
<<badfilter1,cache=TRUE, fig.show = 'hide'>>=
badfilter = as.numeric(gsub("[+]*FBgn", "", rownames(res)))
@
<<badfilter2,echo=FALSE>>=
stopifnot(!any(is.na(badfilter)))
@
<<figbadfilter, fig.show = 'hide'>>=
plot(rank(badfilter)/length(badfilter), -log10(res$pvalue), pch=16, cex=0.45)
@
\begin{figure}[ht]
\centering
\includegraphics[width=.49\textwidth]{figure/figbadfilter-1}
\caption{Scatterplot analogous to Figure~\ref{figscatterindepfilt}, but with \Robject{badfilter}.}
\label{figbadfilter}
\end{figure}

%--------------------------------------------------
\subsection{How to choose the filter statistic and the cutoff?}\label{sec:indepfilterchoose}
%--------------------------------------------------
The \texttt{filtered\_p} function in the \Rpackage{genefilter} package
calculates adjusted $p$-values over a range of possible filtering
thresholds. Here, we call this function on our results from above and
compute adjusted $p$-values using the method of Benjamini and Hochberg (BH)
for a range of different filter cutoffs.
%
\begin{figure}[tb]
\begin{center}
\includegraphics[width=0.49\textwidth]{figure/figrejection-1}
\includegraphics[width=0.49\textwidth]{figure/fignumreject-1}
\caption{Left panel: the plot shows the number of rejections (i.\,e.\ genes detected as
  differentially expressed) as a function of the FDR threshold
  ($x$-axis) and the filtering cutoff $\theta$ (line colours,
  specified as quantiles of the distribution of the
  filter statistic). The plot is produced by the \texttt{rejection\_plot}
  function. Note that the lines for $\theta=0\%$ and
  $10\%$ are overplotted by the line for $\theta=20\%$, since for the
  data shown here, these quantiles correspond all to the same set of
  filtered genes (cf.~Figure~\ref{figscatterindepfilt}). Right panel:
  the number of rejections at FDR=10\% as a function of
  $\theta$.}
\label{figrej}
\end{center}
\end{figure}
%

<<pBH1,cache=TRUE>>=
theta = seq(from=0, to=0.5, by=0.1)
pBH = filtered_p(filter=res$filterstat, test=res$pvalue, theta=theta, method="BH")
@

<<pBH2>>=
head(pBH)
@
%

The rows of this matrix correspond to the genes (i.\,e., the rows of \Robject{res}) and
the columns to the BH-adjusted $p$-values for the different possible
choices of cutoff \Robject{theta}. A value of \Robject{NA} indicates
that the gene was filtered out at the corresponding filter cutoff.
The \Rfunction{rejection\_plot} function takes such a matrix and
shows how rejection count ($R$) relates to the choice of cutoff for the
$p$-values. For these data, over a reasonable range of FDR cutoffs, increased
filtering corresponds to increased rejections.
%
<<figrejection,fig.width=5.5,fig.height=5.5, fig.show = 'hide'>>=
rejection_plot(pBH, at="sample",
               xlim=c(0, 0.5), ylim=c(0, 2000),
               xlab="FDR cutoff (Benjamini & Hochberg adjusted p-value)", main="")
@
The plot is shown in the left panel of the Figure.


%------------------------------------------------------------
\subsubsection{Choice of filtering cutoff}\label{choose:cutoff}
%------------------------------------------------------------
If we select a fixed cutoff for the adjusted $p$-values, we can also look more closely at
the relationship between the fraction of null hypotheses filtered and the total number of
discoveries. The \texttt{filtered\_R} function wraps \texttt{filtered\_p} and just returns
rejection counts. It requires you to choose a particular $p$-value cutoff, specified
through the argument \Robject{alpha}.
%
<<filtered_R1,cache=TRUE>>=
theta = seq(from=0, to=0.8, by=0.02)
rejBH = filtered_R(alpha=0.1, filter=res$filterstat, test=res$pvalue, theta=theta, method="BH")
@

Because overfiltering (or use of a filter which is inappropriate for the
application domain) discards both false and true null hypotheses, very large
values of $\theta$ reduce power in this example:

<<fignumreject,fig.width=5.5,fig.height=5.5, fig.show = 'hide'>>=
plot(theta, rejBH, type="l",
     xlab=expression(theta), ylab="number of rejections")
@
The plot is shown in the right panel of Figure~\ref{figrej}.

%------------------------------------------------------------
\subsubsection{Choice of filtering statistic}\label{choose:filterstat}
%------------------------------------------------------------
We can use the analysis of the previous section~\ref{choose:cutoff} also to inform
ourselves about different possible choices of filter statistic. We construct a dataframe
with a number of different choices.

<<differentstats,cache=TRUE>>=
filterChoices = data.frame(
  `mean`   = res$filterstat,
  `geneID` = badfilter,
  `min`    = rowMin(counts(cds)),
  `max`    = rowMax(counts(cds)),
  `sd`     = rowSds(counts(cds))
)
rejChoices = sapply(filterChoices, function(f)
  filtered_R(alpha=0.1, filter=f, test=res$pvalue, theta=theta, method="BH"))
@

<<colours,results='hide'>>=
myColours = brewer.pal(ncol(filterChoices), "Set1")
@

<<figdifferentstats,fig.width=5.5,fig.height=5.5, fig.show = 'hide'>>=
matplot(theta, rejChoices, type="l", lty=1, col=myColours, lwd=2,
        xlab=expression(theta), ylab="number of rejections")
legend("bottomleft", legend=colnames(filterChoices), fill=myColours)
@
%
The result indicates that for the data at
hand, \Robject{mean}, \Robject{max} and \Robject{sd} provide similar performance, whereas
the other choices are less effective.
\begin{figure}[h!tb]
\begin{center}
\includegraphics[width=0.49\textwidth]{figure/figdifferentstats-1}
\caption{The number of rejections at FDR=10\% as a function of
  $\theta$ (analogous to the right panel in Figure~\ref{figrej}) for a number of different choices of the filter statistic.}
\label{figdifferentstats}
\end{center}
\end{figure}


\section{Weighting as an extension of filtering --- IHW}

In previous section, we introduced gene--wise filtering, which in or excludes genes based on a
filter statistic. This can be seen as p--value weighting wiht  0/1 weights, wher all p-values
either get a weight of 0 or 1.

Similarly, we can try to choose continous weights for the p--values in order to increase the
detection power. IHW (Independent Hypothesis Weighting) tries to achieve exactly this: In addition
to the p--values it allows you to specify a covariate for each test.
The covariate should be informative of the power or prior probability of each individual test,
but is chosen such that the p-values for those hypotheses that are truly null do not
depend on the covariate, see \href{http://biorxiv.org/content/early/2015/12/13/034330}{
Data-driven hypothesis weighting increases detection power in big data analytics}.

The the input of IHW is the following:

\begin{itemize}
    \item  a vector of p-values (of length $m$)
    \item  a matching vector of covariates
    \item  the significance level $\alpha \in (0,1)$ at which the False Discovery Rate should be controlled.
\end{itemize}


IHW then calculates weights for each p-value
(non-negative numbers $w_i \geq 0$ such that $\sum_{i=1}^m w_i = m$).
IHW also returns a vector of adjusted p-values by applying the procedure
of Benjamini Hochberg to the weighted p-values $P^\text{weighted}_i = \frac{P_i}{w_i}$.

The weights allow different prioritization of the individual hypotheses,
based on their covariate. A hypothesis with weight > 1 gets prioritized
in the testing procedure, and the higher the weight the higher the prioritization.
On the other hand, a hypothesis with weight equal to
0 cannot be rejected and essentially is filtered out of the procedure.

We will show now how to use the IHW package in analysing for RNA-Seq
differential gene expression and then also mention some other examples
where the method is applicable. We use \Biocpkg{airway} RNA-Seq dataset
and \Biocpkg{DESeq2} .

\subsection{IHW and the airway RNA--Seq data}

<<Deseq2, message=FALSE, warning=FALSE>>=

dds <- DESeqDataSet(se = airway, design = ~ cell + dex)
dds <- DESeq(dds)
de_res <- as.data.frame(results(dds))
@

The output is a `data.frame` object, which includes the following columns for each gene:

<<>>=
colnames(de_res)
@

In particular, we have p-values and baseMean (i.e., the mean of normalized counts) for each gene.
As argued in the DESeq2  paper, these two statistics are approximately independent under
the null hypothesis. Thus we have all the ingredient necessary for a IHW analysis (p-values and covariates),
which we will apply at a significance level 0.1.

First load IHW:
<<message=FALSE, warning=FALSE>>=
ihw_res <- ihw(pvalue ~ baseMean,  data=de_res, alpha = 0.1)
@

This returns an object of the class `ihwResult`. We can get e.g. the total number of rejections.

<<>>=
rejections(ihw_res)
@

And we can also extract the adjusted p-values:
<<>>=
head(adj_pvalues(ihw_res))
sum(adj_pvalues(ihw_res) <= 0.1, na.rm = TRUE) == rejections(ihw_res)
@

We can compare this to the result of applying the method of Benjamini and Hochberg to the p-values only:

<<>>=
padj_bh <- p.adjust(de_res$pvalue, method = "BH")
sum(padj_bh <= 0.1, na.rm = TRUE)
@

We thus get a lot more rejections! How did we get this power? Essentially it was possible
by assigning appropriate weights to each hypothesis. We can retrieve the weights as follows:

<<>>=
head(weights(ihw_res))
@


\subsection{Computation of the weights}


Internally, what happened was the following: We split the hypotheses into $n$ different strata (here $n=22$)
based on increasing value of baseMean and we also randomly split them into $k$ folds (here $k=5$).
Then, for each combination of fold and stratum, we learned the weights. The discretization into
strata facilitates the estimation of the distribution function conditionally on the covariate
and the optimization of the weights. The division into random folds helps us to avoid "overfitting"
the data, something which can result in loss of control of the False Discovery Rate.

In particular, each hypothesis test gets assigned a weight depending on the combination
of its assigned fold and stratum.

We can also see this internal representation of the weights as a ($n$ X $k$) matrix:

<<>>=
weights(ihw_res, levels_only=TRUE)
@

Finally, IHW contains a convenience function to visualize the estimated weights:

<<plot_ihw_res, fig.width = 5, fig.height = 3>>=
plot(ihw_res)
@

For each hypothesis, one can visually determine its weight by identifying its stratum
and the fold it was assigned to. In the example above, we see that the general trend
is driven by the covariate (stratum) and not as much by the fold. Recall that IHW
assumes that the "optimal" weights should be a function of the covariate
(and hence the stratum) only. Therefore, the weight functions calculated
on random (overlapping) splits of the data should behave similarly,
while there should be no trend driven by the folds. Also as expected,
genes with very low baseMean count get assigned a weight of 0, while genes
with high baseMean count get prioritized.

As a further convenience for further work, a ihwResult object can be converted to a data.frame as follows:

<<>>=
ihw_res_df <- as.data.frame(ihw_res)
colnames(ihw_res_df)
@


\section{Answers to exercises}

\subsubsection*{Exercise: Normal Model for a gene}

Suppose that the distribution of the expression values for a
gene is distributed according to $N(1.6, 0.42)$.

1. Compute the probability that the expression values are less
than 1.2.
2. What is the probability that the expression values are between 1.2
and 2.0?
3. What is the probability that the expression values are between 0.8
and 2.4?
4.  Compute the exact values for the quantiles $x_{0.025}$ and $x_{0.975}$.

5. Use \Rfunction{rnorm} to draw a sample of size 1000 from the population and
compare the sample mean and standard deviation to that of the population.


\subsubsection*{Solution:  Normal Model for a gene}

<< geneExp-Exercise, echo = TRUE, results ='hide' >>=
# a
#######################################################
1 - pnorm(1.2, 1.6, 0.42)

# b
#######################################################
pnorm(2, 1.6, 0.42) - pnorm(1.2, 1.6, 0.42)

# c
#######################################################
pnorm(2.4, 1.6, 0.42)  - pnorm(0.8, 1.6, 0.42)


# d
#######################################################
qnorm(0.025, 1.6, 0.42)
qnorm(0.975, 1.6, 0.42)

# e
#######################################################
test.sample = rnorm(1000, 1.6, 0.42)
mean(test.sample)
sd(test.sample)
@


\xhrulefill{BiocBlue}{1pt}

\subsubsection*{Exercise: Rocky mountain spotted fever}

In 747 cases of "Rocky Mountain spotted fever" from the western United States,
210 patients died.
Out of 661 cases from the eastern United States,
122 died. Is the difference statistically significant?
Use a prop--test as well as a Fisher--test.

\subsubsection*{Solution: Rocky mountain spotted fever}
<<sol-fever,   echo = TRUE, results='hide'>>=
deaths <-c(210,122)
tot.cases <-c(747,661)

prop.test(deaths, tot.cases)

chisq.test(rbind(deaths, tot.cases-deaths))
fisher.test(rbind(deaths, tot.cases-deaths))
@

\xhrulefill{BiocBlue}{1pt}

\subsubsection*{Exercise: Group comparison for gene GYPC}
The gene GYPC plays an important role in regulating the mechanical
stability of red cells. I can be found
in line \Sexpr{grep("GYPC", anno_fusALL$SYMBOL)} of the expALL data set. (Try
\Rfunction{grep("GYPC", anno\textunderscore fusALL\$SYMBOL))}.

Test for the equality of the
means by an appropriate $t$--test. Is the experimental effect very strong?
Also, try testing the hypothesis  using a moderated $t$--test and a wilcoxon
test.

\subsubsection*{Solution: Group comparison for gene GYPC}

<<sol-t-test,   echo = TRUE, results='hide'>>=

t.test(expALL[8197,] ~ groupsALL, var.equal=FALSE)

### strong difference between groups ...
### confirmed by wilcoxon test
wilcox.test(expALL[8197,] ~ groupsALL)

### the moderated t-test is also significant
modt.stat(t(expALL), groupsALL)[8197]
### p-value using the normal distribution
2 - 2*pnorm(abs(modt.stat(t(expALL), groupsALL)[8197]))
@


\xhrulefill{BiocBlue}{1pt}


\subsubsection*{Exercise: Multiple testing for the ALL data}
Try other multiple testing procedures like q--values on the $p$--values
obtained from the shrinkage t statistics. Can you gain power? Produce a $p$--value
histogram where significant statistics are indicated by color-fill.

\subsubsection*{Solution: Multiple testing for the ALL data}
% results='hide
<<sol gol multiple testing, results='hide', fig.keep='none'>>=
golub.qval <- Qvalue(pval.st)
significant <- as.factor(golub.qval$qValues < 0.05)
levels(significant) <- ifelse(levels(significant) , "yes", "no")

table(significant)

ggplot2::qplot(pval.st , xlab = "p-values of shrinkage t statistics",
               main = "Histogram of p-values,
       golub data, q-value < 0.05",
       fill = significant)

significant <- as.factor(pv.st.BH < 0.05)
levels(significant) <- ifelse(levels(significant) , "yes", "no")

(ggplot2::qplot(pval.st , xlab = "p-values of shrinkage t statistics",
                main = "Histogram of p-values,
       golub data, BH adjusted p-value < 0.05",
       fill = significant)  + scale_fill_brewer(type = "qual", palette = 8))
@


\end{document}
