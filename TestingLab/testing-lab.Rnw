
% To compile this document
% graphics.off();rm(list=ls());library('knitr');knit('testing-lab.Rnw');  for(i in 1:2) system('R CMD pdflatex testing-lab.tex'); purl('testing-lab.Rnw')



\documentclass{article}


<<style, echo=FALSE, results='asis'>>=
BiocStyle::latex()
@



<<options, include=FALSE>>=
options(digits=3, width=80)
opts_chunk$set(echo=TRUE,tidy=FALSE,include=TRUE,
               dev='png', fig.width = 6, fig.height = 3.5, comment = '  ', dpi = 300,
		cache = T, lazy.load = FALSE, background="grey93" )
@



\title{Statistical Testing, Including Multiple Testing}
\usepackage[T1]{fontenc}
%\usepackage{enumerate}
\usepackage{amsmath, amssymb, amstext}
\usepackage{natbib}
\usepackage{mathpazo}
\usepackage{soul}
\usepackage{cases}
\setlength{\parindent}{0cm} 
\usepackage{xhfill}
\usepackage[labelformat=empty]{caption}
\providecommand{\abs}[1]{\left\lvert#1\right\rvert}

\author{Bernd Klaus$^1$ \\[1em]European Molecular Biology Laboratory (EMBL),\\ Heidelberg, Germany\\
\texttt{$^1$bernd.klaus@embl.de}}

\begin{document}

\maketitle


\tableofcontents

%--------------------------------------------------
\section{Required packages and other preparations} \label{sec:prep}
%--------------------------------------------------



%
<<required packages and data, echo = TRUE, message = FALSE>>=


library(gplots)
library(RColorBrewer)
library(ggplot2)
library(plyr)
library(dplyr)
library(magrittr)
library(tidyr)
library(mutoss)
library(qvalue)
library(st)
library(ALL)
library(hgu95av2.db)
set.seed(999)
library(genefilter)
@
%


\section{Introduction to statistical hypothesis testing}

In science it is common to ask if two things are different
Are men taller than women? Is the risk of cancer different in smokers and non-smokers? Is 
the probability of getting type II different for different genetic backgrounds? Is this 
gene differentially expressed in cancer? When we make two measurements and compare, we 
almost always see some difference. But will wee see it again if we measure again? If 
someone else measures? Statistical testing can help us answer this question.

Here we deal with questions  related to the statistical testing
biological hypothesis. Does the mean gene expression over ALL patients
differ from that over AML patients? That is, does the mean gene expression
level differ between experimental conditions? Is the mean gene expression
different from zero?   How can it be tested whether the 
frequencies of nucleotide sequences of two genes are
different?  What is the probability of a certain micro RNA to have more than a certain
number of purines?


Many population parameters are used to define
families of theoretical distributions. In any research (empirical) setting the
specific values of such parameters are unknown so that these must be estimated. 
Once estimates are available it becomes possible to statistically test
biologically important hypotheses. This lab gives several basic
examples of statistical testing and some of its background. 

As a conceptual example for a typical testing situation, let $\mu_0$ be a 
number representing the hypothesized population mean by a
researcher on the basis of experience and knowledge from the field. With
respect to the population mean the null hypothesis can be formulated as
$H_0 : \mu = \mu_0$ and the alternative hypothesis as $H_1$ : $\mu \neq \mu_0$ .
These are two statements of which the latter is the opposite of the first: Either 
$H_0$ or $H_1$ is true. The alternative hypothesis is true if $H_1 : 
\mu < \mu_0$ or $H_1 : \mu > \mu_0$ holds true. 
This type of alternative hypothesis is called ``two-sided''. In case
$H_1$ : $\mu > \mu_0$ or $H_1$ : $\mu < \mu_0$, it is called ``one-sided''.


Such a null hypothesis will be statistically tested against the alternative
using a suitable distribution of a statistic (e.g. standardized mean). After
conducting the experiment, the value of the statistic can be computed from
the data. By comparing the value of the statistic with its distribution, the
researcher draws a conclusion with respect to the null hypothesis: $H_0$ is
rejected or it is not. The probability to reject $H_0$, given the truth of $H_0$, is
called the significance level which is generally denoted by $\alpha$. We shall follow
the habit in statistics to use $\alpha = 0.05$, but it will be completely clear how to
adapt the procedure in case other significance levels are desired.

This workflow can be summarized as follows:



\begin{enumerate} 
\item Set up hypothesis $H_0$ (that you want to reject)
\item  Find a test statistic $T$ that should be sensitive to
(interesting) deviations from $H_0$
\item  Figure out the  null distribution of $T$, the distribution of $T$
under the assumption that $H_0$ holds 
\item  Compute the actual value of $T$ for the data at
hand
\item  Compute $p$--value = the probability of observing that
value, or more extreme, assuming the null distribution.
\item Test Decision: Rejection of $H_0$ - yes / no ?
\end{enumerate}



\section{The two group comparison as a fundamental example for testing}


Imagine a researcher who would like to compare the height of two plant varieties. 
If she only takes one measurement of the plant height and observes a difference of say 2cm, 
it is impossible to say whether this difference is due to natural variation. 
On the other hand, if multiple plants of each variety are measured, and it turns out 
that the height differences are always somewhere around 2cm, the observed difference 
is less likely due to chance. This is illustrated in the figure below: 
the difference is strong relative to the variability between the measurements.

The data might look like this:

<<plantData>>=

variety1 <- rnorm(10, mean = 5)
variety2 <- rnorm(10, mean = 7, sd = 1/sqrt(2))

plantData <- data.frame(height  = c(variety1,variety2) , 
                     variety = sort(rep(c("variety1", "variety2"),10)) )

head(plantData)


(qplot(variety, height, data = plantData, color=variety, 
      position=position_jitter(w = 0.3, h = 0), size = I(3), shape = variety) 
    + scale_color_brewer(palette = "Set1") + ylab("height in cm") +
      geom_errorbar(stat=  "hline",  colour="black",yintercept="mean", 
      width=0.6, size=1,aes(ymax=..y.., ymin=..y..)))


@


\subsubsection*{Side Note: Technical vs. biological replicates}

When referring to replicates it is important to distinguish between biological and
technical replicates. Technical replicates refer to experimental samples isolated 
from one biological sample, e.g. extracting RNA from the cells of a mouse and 
then preparing 3 sequencing libraries from this while a biological replication means 
extracting RNA from three different mice for the comparisons of interest. 
It is not sufficient "to pipette an experiment again" since this is not biological, 
but merely a ``technical" replication. In general, technical replicates 
tend to show less variability than biological replicates, thus leading 
to false positive results.  

\subsection{How to test for differences via permutations}

The observed mean difference between the the two varieties 
is \Sexpr{mean(variety2) - mean(variety1)} cm. How easy would it be 
for a difference of \Sexpr{mean(variety2) - mean(variety1)} 
cm minutes to occur just by chance? 

To answer this, we suppose there really is no difference between
the two groups, that variety1 and variety2 are just labels. So what would
happen if we assign labels randomly? How often would a difference like 
\Sexpr{mean(variety2) - mean(variety1)} cm occur?

We'll pool all twenty observations, randomly pick 10 of them to label
basic and label the rest extended, and compute the difference in means
between the two groups. We'll repeat that many times, say ten thousand, to
get the permutation distribution shown below. The observed statistic
\Sexpr{mean(variety2) - mean(variety1)} cm is also shown; the fraction 
of the distribution to the right of that value
is the probability that random labeling would give a difference that
large. 

\textbf{In a permutation test, we obtain the null distribution from the data,
rather than analytically as e.g. in a t--test.}

\subsection{Run the permutation test}



<< permTest, dependson="plantsExample">>=

## helper function to compute permutation p-value

mDiff <- function(data, group){
  data$group <- NULL
  data$group <- group
  tmp <-  data %>% 
    group_by(group)  %>%
    summarize(m = mean(data, na.rm=TRUE))
  as.numeric(tmp[2,"m"] - tmp[1,"m"])
}


## function to compute the permutation test
permTestTwoGroups <- function(group1, group2, 
                              twoSided = TRUE, permutations = 1e4){
  
  stopifnot(is.numeric(group1),  is.numeric(group2),
             length(group1) > 0,  length(group2) > 0,
             is.vector(group1), is.vector(group2),
            is.logical(twoSided))
 
  inputData <- data.frame(data  = c(group1, group2), 
                     group = rep(c("group1", "group2"),
                                   c(length(group1), length(group2))))
                              
  obsDiff  <-  mDiff(inputData, inputData$group)
  
  # compute sampling distribution and p--value
  samplingDist <- c(replicate(as.integer(permutations), 
                              mDiff(inputData, sample(inputData$group))), 
                              obsDiff)
  
  pvalP <- 2*min(1 - ecdf(samplingDist)(abs(obsDiff)), 
                 ecdf(samplingDist)(abs(obsDiff)))
  
  
  samplingDistPlot <- (qplot(samplingDist, fill = I("orange4"),
                             main = "Sampling distribution mean difference",
                              binwidth = 0.1) 
  +  geom_vline(xintercept = obsDiff, size = 2))
    
  return(list(
    samplingDist = samplingDist,
    samplingDistPlot = samplingDistPlot,
    obsDiff = obsDiff,
    pval = pvalP
  ))
  
}

@


We now compute a difference for each label permutation  and plot it. We compute
a \textbf{two-sided p-value} by looking how many of the computed differences are less 
than the observed one of \textbf{mean(variety2) - mean(variety1)} (lower p--value) and
how many are greater than `\textbf{mean(variety2) - mean(variety1)}. The two--sided p--value
corresponds to a test of the null hypothesis that the mean difference between the two varieties
is different from zero.

The lower p--value corresponds to a test of the null hypothesis 
that the mean difference is less than zero while the  upper
p--value corresponds to a test of the null hypothesis 
that the mean difference is greater than zero. The lower and upper p--values
correspond to so--called \textbf{one--sided} tests. 
In order to compute the two--sided p--value,
we compute both one--sided ones and then take twice the smaller one.



<< r getPvalue, dependson="permTest", cache = TRUE >>=
## draw 1e4 times a permutation of the  sample labels  and compute the 
## two sided p--value


testResult <- permTestTwoGroups(group1 = variety1,
                  group2 = variety2)

testResult$pval

testResult$samplingDistPlot

@


In this case, the probability, the p--value, is \Sexpr{testResult$pval}; it would be rare
for a difference this large to occur by chance. The distribution that is shown
in the figure is called a \textbf{sampling distribution}. It describes how the our
 \textbf{test statistic} would be distributed if the \textbf{null hypothesis} was true,
i.e. if there was no difference between the the two varieties. The lower the variability
of the data and the higher the sample size, the "thinner" the sampling distribution will be.

The p--value gives the probability to observe a difference of \Sexpr{testResult$obsDiff} or greater
assuming that the null hypothesis is true. If this probability is very low, we can be confident
that the null hypothesis is not true and thus the alternative is, i.e. that there is actually 
a difference between the height of the two varieties.

The name ``permutation test" stems from the fact that we picked 
$n_1$ observations without replacement to label as the first sample,
and labelled the others as the second sample. This is equivalent to randomly
permuting all labels, hence the name. If we use all possible permutations for the test,
the test is also called an exact test. However, this is computationally unfeasible
for large sample sizes.

\subsection{Summary: two--sample permutation test recipe}

\begin{enumerate} [label=(\emph{\alph*})] 
 \item Pool the values of the two groups
 \item repeat a large number of  times (> 10 000)
    \begin{itemize}
    \item  Draw a resample of size $n_1$ without replacement
  
    \item  Use the remaining $n_2$ observations for the other sample
  
    \item  Calculate the difference in means, or another statistic that com-
    pares samples
    
    \item   Plot a histogram of the random statistic values; show the observed
    statistic.
    
  \item  Calculate the p--value as the fraction of times the random statistics exceed or equal the observed statistic
  \end{itemize}  
\end{enumerate}

  
\section{The two sample $t$--test}

Instead of a permutation test, we can use a t--test to test the difference between the 
two varieties. In contrast to the permutation test, the sampling distribution of the
mean is obtained analytically, via the assumption of a normal distribution for both
input groups. We will discuss the normal distribution next.

\subsection{The Normal Distribution}

The normal distribution is of key importance because it is assumed for many
data generating processes. Among other things, we will look at (reprocessed) 
gene expression values than can be seen as realizations of a random variable $X$ 
having a normal distribution. 

Equivalently, one says that the data values are members of a normally distributed 
population with mean $\mu$ (mu) and variance $\sigma^2$ (sigma squared). It
is good custom to use Greek letters for population properties and $N ( \mu ,\sigma^2 )$
for the normal distribution. The value of the distribution function is given
by $P (X \leq x)$, the probability of the population to have values smaller than
or equal to $x$. Various properties of the normal distribution are illustrated
by the examples below.


\subsection{Example: Explore the Normal Distribution} 

To view members of the normal distribution load the
\CRANpkg{TeachingDemos"} ` package and enter the the command \Rfunction{vis.normal()}
to launch an interactive display of  densities of the normal distribution, 
i.e. bell-shaped curves.
The curves are symmetric around $\mu$ and attain a
unique maximum at $x = \mu$. If $x$ moves further away from the mean $\mu$, then
the curves moves to zero so that extreme values occur with small probability.
Move the mean and the standard deviation from the left to the right to
explore their effect on the shape of the normal distribution. In particular,
when the mean $\mu$ increases, then the distribution moves to the right. If $\sigma$ 
is small/large, then the distribution is steep/flat. 

\subsection{The ALL data}

The ALL data consist of microarrays from 128 different individuals with
acute lymphoblastic leukemia (ALL). There are 95 samples with B-cell ALL
and 33 with T-cell ALL and because these are different tissues and quite
different diseases we consider them separately and focus on the
B-cell ALL tumors. 

An interesting subset, with two groups having approximately the same
number of samples in each group, is the comparison of the B-cell tumors
found to carry the BCR/ABL mutation to those B-cell tumors with no
observed cytogenetic abnormalities. These samples are labeled BCR/ABL
and NEG in the mol.biol covariate. The BCR/ABL mutation, also known
as the Philadelphia chromosome, was the first cytogenetic aberration that
could be associated with the development of cancer, leading the way to the
current understanding of the disease. In tumors harboring the BCR/ABL
translocation a short piece of chromosome 22 is exchanged with a segment
of chromosome 9. As a consequence, a constitutively active fusion protein
is transcribed which acts as a potent mitogene, leading to uncontrolled cell
division.
Not all leukemia tumors carry the Philadelphia chromosome; there
are other mutations that can be responsible for neoplastic alterations of
blood cells, for instance a translocation between chromosomes 4 and 11
(ALL1/AF4). 

From the data, we look at the expression of the gene BCL2. The following
code chunk shows the preprocessing of the data. We only select the B-Cell
tumors and focus on the ones with/without a translocation.

<<setupALL >>=

data("ALL")
bALL <- ALL[, substr(ALL$BT,1,1) == "B"]
fusALL <- bALL[, bALL$mol.biol %in% c("BCR/ABL", "NEG")]
fusALL$mol.biol <- factor(fusALL$mol.biol)
fusALL
sample_n(pData(fusALL), 10)

groupsALL <- fusALL$mol.biol 
expALL <- exprs(fusALL)
  
anno_fusALL <- plyr::ddply(AnnotationDbi::select(hgu95av2.db, 
                                  keys=rownames(expALL), 
                                  columns = c("SYMBOL", "GENENAME", "ENSEMBL"),
                                  keytype="PROBEID"), "PROBEID", function(X){X[1,]})



@



Now the the sample group is coded in the vector \Rfunction{groupsALL}, the expression 
data is in \Rfunction{expALL} and the annotation of the probes is in 
\Rfunction{anno\textunderscore fusALL}

<< showALL, dependson="setupALL" >>=

head(groupsALL)
head(expALL[, 1:5])
head(anno_fusALL)


@



\subsection{Example: A Normal Model for gene expression of BCL2}

Here we look at the gene expression values for the gene BCL2, which is in row
1152 of the data set.
<< getBCL2 >>=
anno_fusALL[1152,]
@


This gene encodes an integral outer mitochondrial membrane protein 
that blocks the apoptotic death of some cells such as lymphocytes. 
Constitutive expression of BCL2 is thought to be the cause of
follicular lymphoma. We now develop a normal distribution model for
the translocation group of the ALL data.



Suppose that the expression  values of the ALL group 
of gene BCL2  can be represented by $X$ which is distributed as 
$N (8.6, 0.5 )$. From
the graph of its density function, it can be observed that it
is symmetric and bell-shaped around $\mu = 8.6$. 

A density function may very well be seen as a histogram with
 arbitrarily small bars (intervals). The
probability that the expression values are less than 8 is
$P (X < 8) =$ \Rfunction{pnorm(8, 8.6, 0.5)} $=$ \Sexpr{pnorm(8, 8.6, 0.5)}. 


The figure next to it illustrates the value 0.115 of the 
\textbf{cumulative distribution function (cdf)}
at $x = 8$. It corresponds to the area of the green colored surface below the graph of the
density function in the figure. 
 

<< BCL2-density-cdf, echo = TRUE >>=

f <-function(x){dnorm(x, 8.6, 0.5)}
F <-function(x){pnorm(x, 8.6, 0.5)}

x <- seq(6,11,0.01)
dataGG <- data.frame(x = x, y = f(x))
dataGG <- mutate(dataGG,  area = ifelse(x < 8, "in", "out"  ))

p<-qplot(data = dataGG, x = x, y = y, geom="line")
p<-p + geom_area(aes(ymax = y, fill = area)) +  guides(fill=FALSE)
p<-p + xlab("Gene-Expression") + ylab("density f") + annotate("text", x = 8, y = 0, label = "8")
p + labs(title = "P(X<=8)= 0.115") + scale_fill_brewer(palette = "Dark2")


dataGG = data.frame(x = x, y = F(x))
dataGG <- mutate(dataGG,  area = ifelse(x < 8, "in", "out"  ))

p<-qplot(data = dataGG, x = x, y = y, geom="line")
p<- p + annotate("text", x = 8, y = 0, label = "8") 
p<- p	+ annotate("text",y = .16, x = 6, label = "0.115")

p <- p + geom_segment(y=0, yend = F(8), x=8, xend = 8, size = I(1))
p <- p + geom_segment(y=F(8), yend = F(8), x=6, xend = 8, size = I(1))
p + labs(title = "P(X<=8)= 0.115") + xlab("Gene-Expression") + ylab("distribution F")
  

@


The probability that the expression values
are greater than $9$ is $P (X \geq 9) =$ 


<< BCL2-1, echo = TRUE >>=
1 - pnorm(9, 8.6, 0.5) 
@


The probability that $X$ is between $8$ and $9$ equals
$P (8 \leq X \leq 9) =$


<< BCL2-2, echo = TRUE >>=
pnorm(9, 8.6, 0.5) - pnorm(8, 8.6, 0.5) 
@



The graph of the distribution function  shows that it is
strictly increasing. For example, the exact value for the quantile $x_{0.025}$ can be computed
by

<< BCL2-3, echo = TRUE >>=
qnorm(0.025,8.6,0.5)
@



That is, the quantile $x_{0.025}$ = 0.92. Hence, it holds that the probability of
observing values less than $0.92$ equals $0.025$, that is $P (X \leq 0.92) = 0.025$,
as can be verified by ` pnorm(0.92, 8.6, 0.5)`.

 When $X$ is distributed as
$N (8.6, 0.5 )$, then the population mean is $8.6$ and the population standard
deviation $0.5$. To verify this we draw a random sample of size $1000$ from this
population by

<<  BCL2-4, echo = TRUE >>=
x <- rnorm(1000,8.6,0.5)
@


The estimates

<< BCL2-5, echo = TRUE >>=
mean(x)
#and 
sd(x)
@

are close to their population values $\mu = 8.6$ and $\sigma = 0.5$.

\subsubsection*{Exercise: Normal Model for a gene} 

Suppose that the distribution of the expression values for a
gene is distributed according to $N(1.6, 0.42)$.

1. Compute the probability that the expression values are less
than 1.2.
2. What is the probability that the expression values are between 1.2
and 2.0?
3. What is the probability that the expression values are between 0.8
and 2.4?
4.  Compute the exact values for the quantiles $x_{0.025}$ and $x_{0.975}$.

5. Use \Rfunction{rnorm} to draw a sample of size 1000 from the population and
compare the sample mean and standard deviation to that of the population.




\subsection{Conducting a t-test}

Suppose that gene expression data from two groups of patients (experimental conditions) 
are available and that the hypothesis is about the difference
between the population means $\mu_1$ and $\mu_2$ . In particular, $H_0 : \mu_1 = \mu_2$
is to be tested against $H_1 : \mu_1 \neq \mu_2$. These hypotheses can also be formulated
as $H_0 : \mu_1 - \mu_2 = 0$ and $H_1 : \mu_1 - \mu_2 \neq 0$. Suppose that gene expression
data from the first group are given by $\{x_1 , \dotsc, x_n \}$ and that of the second by
$\{y_1 , \dotsc, y_m \}$. Let $\bar x$ be the mean of the first and $\bar y$ that of the second, 
$s_1$ the variance of the first and  $s_2$ that of the second. Then the $t$-statistic can
be formulated as

$$
t = \frac{ \bar x - \bar y -(  \mu_1 - \mu_2 ) }{ s_1/\sqrt{n} + s_2/\sqrt{m}}
$$


The decision procedure with respect to the null-hypothesis is completely analogous
to the permutation test. However, the sampling distribution is 
\textbf{found analytically based on assumptions on the data and not by permutations}.

Note that the $t$--value is large if the difference between
$x$ and $y$ is large, the standard deviations $s_1$ and $s_2$ are small, and the sample
sizes are large. This means for example that higher sample sizes allow you to detect
more subtle mean differences. The $t$--test  with the assumptions of unequal variances
in the groups is also known as the Welch two-sample $t$--test and is routinely performed.

If $s_1 = s_2$ the variance estimator of the $t$--test and the calculation of the degrees
of freedom of the $t$--distribution changes slightly. This test is available by specifying
\Rfunction{var.equal = TRUE} when calling the function \Rfunction{t.test}.

\subsubsection*{Example: Comparing BCL2 between BCR/ABL and NEG  }
The  gene BCL2 plays an important role with respect to discriminating
BCR/ABL from NEG patients. The null hypothesis of equal means can be tested by the function 
\Rfunction{t.test} and the appropriate factor and specification to separate the groups.
(\Rfunction{var.equal=FALSE}  by default).

<<tTestBCL2,  echo = TRUE>>=
t.test(expALL[1152,] ~ groupsALL)
### alternative call
t.test(expALL[1152, groupsALL == "BCR/ABL"], 
       expALL[1152, groupsALL == "NEG"] )
@
The $t$-value is quite large, indicating that the two means $x$ and $y$ differ largely
from zero relative to the corresponding standard error . Since the $p$--value is extremely small, 
the conclusion is to reject the null--hypothesis of equal means. The data provide 
strong evidence that the population means do differ. 



\section{Wilcoxon rank test} 
In case the data are normally distributed with equal variance, the $t$--test is
an optimal test for testing $H_0 : \mu_1 = \mu_2$  against $H_1 : \mu_1 \neq \mu_2$.
 If, however, the data are not normally distributed due to skewness or
otherwise heavy tails, then this optimality does not hold anymore and there
is no guarantee that the significance level of the test equals the intended
level $\alpha$. Usually, one will  loose power if the normality assumption 
is validated, i.e. the  $\alpha$ will be inflated. 

For this reason rank type of tests are developed for which on beforehand 
no specific distributional assumptions need to be made.
In the example below we shall concentrate on the two-sample Wilcoxon test. 


To broaden our view we switch from hypotheses about means to those about distributions. 
An alternative hypothesis may then be formulated as that the distribution of a first 
group lays to the
left of a second.  


To set the scene let the gene expression values of the first
group ($x_1$ to $x_m$) have distribution $F$ and 
those of the second group ($y_1$ to $y_n$) distribution $G$. 
The null hypothesis is that both distributions are equal
($H_0 : F = G$) and the alternative that they are not. 

For example that the $x$'s are smaller (or larger) than the $y$'s. 
By the two-sample Wilcoxon test the data $x_1 , \dotsc , x_m$ , 
$y_1 , \dotsc , y_n$ are ranked and the rank numbers of the $x$'s are
summed to form the statistic $W$ after a certain correction. 

The idea is that if the ranks of $x$'s are smaller than those of the $y$'s, then the
sum is small. The distribution of the sum of ranks is known so that a $p$--value
can be computed on the basis of which the null hypothesis is rejected if it is
smaller than the significance level  $\alpha$.




\subsubsection*{Example: BCL2 gene}
The null hypothesis that the expression values for gene
BCL2  are equally distributed for the ALL patients and the AML
patients can be tested by the built--in--function \Rfunction{wilcox.test}, as follows.



<< wilcoxTestBCL2,  echo = TRUE>>=
wilcox.test(expALL[1152,] ~ groupsALL)
@

Since the $p$--value is much smaller than $\alpha = 0.05$, the conclusion is to reject
the null--hypothesis of equal distributions.


\subsubsection*{A permutation test for BCL2}

We can of course also test the BCL2 gene for differential expression using
a permutation test:

<<  permTestBCL2 >>=
pTest <- permTestTwoGroups(expALL[1152, groupsALL == "BCR/ABL"], 
       expALL[1152, groupsALL == "NEG"], permutations = 1e4 )

pTest$pval

pTest$samplingDistPlot
@

The result is similar to the other tests performed.

\subsection{Where permutation test do not apply}

Permutation tests are helpful and their widespread use has been made possible by 
the computer power we have at our disposal today. However, they are not a panacea. 

We have seen that in the two groups case,  permutation testing is straightforward. 
The same is true for the testing of the dependence between two variables for example, while 
other situations are not easily covered by a permutation test, such testing a single
sample mean. Also, we cannot perform a permutation test of the mean difference if the variance
in the two groups differ, since then we cannot pool the data.



\subsection{Caveat: Wilcoxon test vs. t--test}

The t--test requires normally distributed data in order to be valid. In practice,
this leads to many people prefer Wilcoxon tests over the t--test. However,
the problem with the Wilcxocon test is that it implicitly assumes \textbf{equal variances} 
in the two groups, since it only  tests for shifts in location. 
So if the variances are not equal, it can give misleading results. 

It actually often leads to overly low p--values. Below is a little simulation study 
showing this effect. Two groups of 10 normally distributed values are simulated,
one with a standard deviation of 1 and another with a standard deviation of 15.
There is no difference between the groups (both have mean 0), although 
the standard deviations are very different, so we expect a
proportion of 5\%  significant p--values at an $\alpha$--level of 5\%.

<< WilcoxSim>>=


x <- rnorm(10)
qqnorm(x)
qqline(x)


y <- rnorm(10)
wilcox.test(x,y)



wc <- function(){
  x <- rnorm(10)
  y <- rnorm(10, sd = 15)
  tt <- wilcox.test(x,y)
  tt$p.value
}

pVals_WC <- replicate(1000, expr = wc()) 

hist(pVals_WC, col = "tan3", main = "Wilcoxon P--values")
prop.table(table(pVals_WC < 0.05))


set.seed(999)

ttest <- function(){
  x <- rnorm(10)
  y <- rnorm(10, sd = 15)
  tt <- t.test(x,y)
  tt$p.value
}

pValsT <- replicate(1000, expr = ttest()) 

hist(pValsT, col = "coral3", main = "t-test P--values")
prop.table(table(pValsT < 0.05))

@


the t--test pvalues are correct (uniformly distributed) and
the alpha level is kept, while the Wilcoxon test is too optimistic and
has an actual level of near 10\% at a nominal level
of 5\%. So there are too many false positives for the Wilcoxon test.
 
 
 

\section{Chi--squared Test and the fisher test for contingency tables}

The test above treat continuous data, We now turn to tests for categorical data.
Typically, categorical data is represented in the form of contingency
tables, where one categorization is represented by the rows and the other 
by the columns. A $\chi^2$ test then tests for a  for independence of
rows and columns in an $r \times c$ contingency table. It will tell us, whether 
the row classifications are independent of the column classifications in a table like
this:



\begin{center}
\includegraphics[width=6cm,height=3.7cm]{cont-table}
\end{center}

The actual number observations in each cell of the table can be 
compared to the expected number of observations under the assumption
of independent row and column classifications and is given by 

\begin{center}
\includegraphics[height = 1.5cm, width = 5cm]{observed-cells}
\end{center}

and a $\chi^2$ statistic can be computed as above:
\begin{gather*}
\chi^2 = \sum_{i=1}^{r} \sum_{j=1}^{c} {(O_{ij} - E_{ij})^2 \over E_{ij}}
\end{gather*}
it has $(r - 1)(c - 1)$  degrees of freedom. 

\subsection{Fishers tea tasting experiment and genetics}

One of the most famous examples of hypothesis testing was performed by RA Fisher on a lady 
that claimed could tell if milk was added before or after the tea was poured. 
Fisher gave the lady four pairs of cups of tea: 
one with milk poured first, the other after. The order was randomized.
Say the lady picked 3 out 4 correctly, do we believe she has a special ability? Tests
for discrete data help to answer this question by quantifying what happens by chance.

The basic question we ask is: if the lady is just guessing, what are the chances that she 
gets 3 or more correct?  If we assume the lady is just guessing randomly, we can think of 
this particular examples as picking 4 balls out of an urn with 4 green (correct answer) and 
4 red (incorrect answer) balls. 

Under the null hypothesis that the lady is just guessing each ball has the same chance of 
being picked. We can then use combinatorics to figure out the probability. The probability 
of picking 3 is 
${4 \choose 3} {4 \choose 1} / {8 \choose 4} = 16/70$.  
The probability of picking all correct is
${4 \choose 4} {4 \choose 0}/{8 \choose 4}= 1/70$. Thus the chance of observing a 3 or 
something more extreme, under the null hypothesis, is 0.24. This is the $p$--value. 
This is called Fisher's exact test and it uses the hyper geometric distribution. It is not 
appropriate for most the tests applied in genetics but the idea is similar.

For example, imagine we have 250 individuals, some of them have a given disease 
others don't. We observe that a 20\% of the individuals that are homozygous for the minor allele have the disease compared to 10\% of the rest. Would we see this again if we picked another 250 individuals?

Here is an example dataset
<<Gentetic association test>>=
disease=c(rep("no",180),rep("yes",20),rep("no",40),rep("yes",10))
genotype=c(rep("AA",200),rep("aa",50))
tab=table(genotype,disease)
tab
@

The null-hypothesis is that the 200 and 50 individuals in each group were assigned disease with the same probability. If this is the case then the probability of disease is
<<Gentetic association test 2>>=
p <- mean(disease == "yes")
p
@
The expected table is therefore
<<Gentetic association test 3>>=
rbind(c(1-p,p)*sum(genotype=="aa"),c(1-p,p)*sum(genotype=="AA"))
@

We can compute an $\chi^2$ statistic of seeing a deviation for the expected table 
as big as this one.
The p-value for this table is 
<<Gentetic association test 4>>=

chisq.test(tab)$p.value
@

Note that there is not a one to one relationship between the odds ratio
($\frac{n_{11}}{n_{12}} /  \frac{n_{21}}{n_{22}} $ )
and the p-value. If we increase the numbers but keep the difference 
in proportions the same, the $p$--value is reduced substantially:

<<Gentetic association test 5>>=
tab=tab*10
chisq.test(tab)$p.value
@


\subsection{Simple gene set enrichment analysis}
Suppose that the number of  onco--type of genes in Chromosome 1
is $n_{11} = 100$ out of a total of $n_{12} = 200$ genes 
and the number of onco--genes in the rest of the genome 
is $n_{21} = 300$ out of a total of $n_{22} = 6000$ genes as summarized in the
table. 

\begin{table}[h!t]
\centering
\begin{tabular}{|r|rr|c|}
  \hline
 & onco--genes & non--onco-genes & row--sums \\ 
  \hline
Chromosome 1 & 100 & 200 & 300\\ 
Rest of Genome & 3000 & 6000 & 9000 \\ 
   \hline
 column--sums & 3100 & 6200 & 9300 \\
   \hline
\end{tabular}
\end{table}

The $\chi^2$ test will now tell us, whether there is a significantly higher 
or lower proportion of onco--genes in chromosome 1 than in the  rest of the genome.
Chromosome 1 serves as our gene set here.
In biology, over--representation is often called ``enrichment'' and an under--representation 
is called ``depletion'' and hence the  $\chi^2$ test for this table can be viewed 
as test of an onco--gene enrichment/depletion in the gene set chromosome 1:

<<chi2-enrich>>=
dat1 <- matrix(c(100,200,3000,6000),2,byrow=TRUE)
## Chi2 test
chisq.test(dat1)
@
An alternative to the $\chi^2$ test for 2 $\times$ 2 tables is the 
 Fisher--test. It tests whether
the odds ratio $\frac{n_{11}}{n_{12}} /  \frac{n_{21}}{n_{22}} $ is significantly
different from 1, which would again indicate a ``depletion'' (if OR $< 1$) or enrichment 
(if OR  $>1$) of oncogenes in chromosome 1 in this case. 
As we can see the odds ratio is 1, i.e. there is neither enrichment nor depletion. 

<<chi2-enrich-fisher>>=
dat1 <- matrix(c(100,200,3000,6000),2,byrow=TRUE)

## Fisher test
fisher.test(dat1)

@


As yet another alternative, we can compare the proportion of onco--genes in chromosome
1 to the the proportion of  onco--genes in the rest of the genome. As we can see,
the test of proportions also returns a $p$--value of 1. 


<<chi2-enrich-prop>>=
dat1 <- matrix(c(100,200,3000,6000),2,byrow=TRUE)
## Comparison of proportions of oncogenes in the two subsets
dat1.prop <- matrix(c(dat1[1,1], dat1[2,1],dat1[1,1] + dat1[1,2], 
               dat1[2,1] + dat1[2,2]), 2,2,byrow=TRUE) 
prop.test(dat1.prop[1,] ,dat1.prop[2,])

@



Let's look at some additional data: the table below shows an example of an 
under--representation of onco--genes in Chromosome 1 

\begin{table}[h!t]
\centering
\begin{tabular}{|r|rr|c|}
  \hline
 & onco--genes & non--onco-genes & row--sums \\ 
  \hline
Chromosome 1 & 50 & 250 & 300 \\ 
Rest of  Genome & 3000 & 6000 & 9000 \\ 
   \hline
 column--sums & 3050 & 6250 & 9300 \\
   \hline
\end{tabular}
\end{table}

%

$\chi^2$ test
<<chi2-enrich-2>>=
dat2 <- matrix(c(50,250,3000,6000),2,byrow=TRUE)
## Chi2 test
chisq.test(dat2)
## Fisher test
fisher.test(dat2)
## Comparison of proportions of oncogenes in the two subsets
dat2.prop <- matrix(c(dat2[1,1], dat2[2,1],dat2[1,1] + dat2[1,2], 
               dat2[2,1] + dat2[2,2]), 2,2,byrow=TRUE) 
prop.test(dat2.prop[1,] ,dat2.prop[2,])
@
%

Both the $\chi^2$ test as well as the Fisher test are significant. 
The odds ratio is $\frac{50}{250} /  \frac{3000}{6000}  = 0.4$
showing a depletion (OR $< 1$) of oncogenes in chromosome 1. 
The test of proportions also gives a significant $p$--value.

The odds ratio is very often used as a measure of association in 
2 $\times$ 2 tables since the absolute value of the natural logarithm
of the odds ratio (often called lod--score) given by 

\begin{gather*}
\text{lod} = \abs{\ln\left(\frac{n_{11}}{n_{12}} /  \frac{n_{21}}{n_{22}}\right)}
\end{gather*}

is only dependent on the cell contents
of table, i.e. shuffling rows or columns does not change it.

%ex.df <- data.frame( significant = c(100,300), non_significant = c(2000,6000), 
%row.names = c("Chromosome_1", "genome") )
%print(xtable(ex.df  ), type = "latex")


\subsubsection*{Exercise: Rocky mountain spotted fever}

In 747 cases of "Rocky Mountain spotted fever" from the western United States, 
210 patients died. 
Out of 661 cases from the eastern United States, 
122 died. Is the difference statistically significant?
Use a prop--test as well as a Fisher--test.

\section{Multiple testing}

When performing a large number of tests, the Type I error is inflated:
Let's assume we perform  $m$ tests with Type I error rate $\alpha$ (reject $H_0$ 
although  $H_0$ is true) of 5\%. Then the probability of  \textbf{no false rejection} 
if the tests are independent is:

\begin{gather}
\underbrace{0.95\cdot{0.95}\cdot \dotso \cdot {0.95}}_\text{m--times} \ggg  0.95
\end{gather}

Thus, the larger the number of tests performed, the higher the probability 
of a false rejection (= Type I error, false positive)

However, this problems is often put aside and hypothesis testing/significance 
analysis is commonly used in a too simple way. Correcting for multiple testing
helps to avoid false positives or discoveries. There are two key components
of a multiple testing procedure:
\begin{itemize}
  \item Error measure
  \item Correction procedure / estimation algorithm
\end{itemize}


\subsection{Types of errors and error rates}

Suppose you are testing a hypothesis that a parameter $\beta$ equals zero versus 
the alternative that it does not equal zero. Let us assume that there are
$m_0$ number of tests that correspond to a true null hypothesis out of $m$ total
tests and that we reject $R$ null hypotheses in total. These are the possible outcomes:


\begin{table}[h!t]
\centering
\begin{tabular}{|r|rr|c|}
  \hline
 & $\beta=0$  & $\beta\neq0$ & Hypotheses  \\ 
  \hline
Claim $\beta=0$  & True Positive  & False Negative & $m-R$\\ 
Claim $\beta\neq 0$  & False Positive & True Negative  &  $R$ \\ 
   \hline
Claims  & $m_0$  & $m-m_0$ & $m$\\
   \hline
\end{tabular}
\end{table}

\begin{itemize}
  \item Type I error or false positive  --- Say that the parameter
        does not equal zero when it does
  \item Type II error or false negative  --- Say that the parameter 
  equals zero when it doesn't 
\end{itemize}

Just like ordinary significance testing tries to control the false positive
rate, there are other types of rates commonly used in multiple testing procedures:

\begin{itemize}
  \item \textbf{False positive rate} - The rate at which false results ($\beta = 0$) are
called significant: $E\left[\frac{FP}{m_0}\right]$

  \item \textbf{Family wise error rate (FWER)} - The probability of at least 
one false positive ${\rm Pr}(FP \geq 1)$

  \item \textbf{False discovery rate (FDR)} - The rate at 
  which claims of significance are false  $E\left[\frac{FP}{FP + TP}\right]$
\end{itemize}


If $p$--values are correctly calculated calling all $p < \alpha$ significant 
will control the false positive rate at level $\alpha$ on average. 



\subsection{Control of  error rates}

Suppose that you perform 10,000 tests and $\beta = 0$ for all of them. 
and  you call all $P < 0.05$ significant. 
Then expected number of false positives is: $10,000 \times 0.05 = 500$  
false positives. How do we avoid so many false positives? 

\subsubsection{Controlling family-wise error rate (FWER)}

The Bonferroni correction is the oldest multiple testing correction. 

\underline{Basic algorithm}
\begin{itemize}
    \item Suppose you do $m$ tests
  \item You want to control FWER at level $\alpha$ so $Pr(FP \geq 1) < \alpha$
  \item Calculate $p$--values normally
  \item Set $\alpha_{fwer} = \alpha/m$
  \item Call all $p$-values less than $\alpha_{fwer}$ significant
\end{itemize}

The bonferroni correction is easy to calculate but very conservative.


\subsubsection{Controlling false discovery rate (FDR)}

This is the most popular correction when performing lots of tests as in in genomics.
It is often termed the Benjamini Hochberg procedure and controls
the FDR.

\underline{Basic algorithm }

\begin{itemize}
  \item Suppose you do $m$ tests
  \item You want to control FDR at level $\alpha$ so
  $E\left[\frac{FP}{TP + FP}\right] < \alpha$
  \item Calculate $p$--values normally
  \item Order the $p$--values from smallest to largest $p_{(1)},...,p_{(m)}$
  \item Call any $p_{(i)} \leq \alpha \times \frac{i}{m}$ significant
\end{itemize}

The FDR control procedure is still pretty easy to calculate and
less conservative (possibly much less) than  controlling the FWER.
On the contrary, it  allows for more false positives and
may behave strangely under dependence.


\subsubsection{Adjusted $p$--values}

The  approach indicated above is to adjust the threshold $\alpha$,  a different 
approach is to calculate ``adjusted $p$--values". They are not $p$--values anymore but 
they can be used directly without adjusting $\alpha$.

Example

\begin{itemize}
  \item Suppose p-values are $p_1,\ldots,p_m$
  \item You could adjust them by taking $p_i^{fwer} = \max\{m \times p_i,1\}$ for each $p$--value.
  \item Then if you call all $p_i^{fwer} < \alpha$ significant you will control the FWER. 
\end{itemize}

\subsection{Diagnostic plots for multiple testing procedures}
The code below simulates  $m = 200$ $p$--values from the mixture model 

\begin{gather*}
0.75\cdot N(0,1) + 0.25 \cdot N(2,1)
\end{gather*}

,i.e. $m_0 = 150$ here and the null distribution is 
the standard normal distribution. \textbf{It is an important fact hat for a 
continuous null distributions the corresponding $p$--values are uniform}. 

<<simulate z scores>>=

sd.true =1
eta0.true = 0.75

get.random.zscore = function(m=200)
{
  
  m0 = m*eta0.true
  m1 = m-m0
  z = c(  rnorm(m0, mean=0, sd=sd.true),
       rnorm(m1, mean=2, sd=1))
  #z = sign(rnorm(length(z)))*z

  return(z)
}

set.seed(555)
z <- get.random.zscore(200)
pv = 1- pnorm(z, sd = sd.true) 

@

\subsubsection{Schweder and Spj\o{}tvoll plot}


If a test statistic does not correspond to a true null hypothesis, the corresponding 
$p$--value will be very small. 
For large $p$--values which likely  will 
correspond to true null hypotheses it 
then holds that
\begin{gather*}
E[M(p)] = m_0(1-p)  \, .
\end{gather*}

Large (probably non--null) $p$--values will thus be close to a straight 
line with slope $m_0$. Accordingly, small 
$p$--values (probably null) will deviate from that line. 

Schweder and Spj\o{}tvoll (Biometrika, 1982) suggested to use these
facts for a diagnostic plot
of the observed $p$-values which permits estimation of the fraction of true null
hypotheses. For a series of hypothesis tests $H_1, \ldots, H_m$ with $p$-values
$p_i$, they suggested plotting
%
\begin{equation*}
  \left( 1-p_i, M(p_i) \right) \mbox{ for } i \in 1, \ldots, m,
\end{equation*}
%
where $M(p)$ is the number of $p$--values greater than $p$. An application of
this diagnostic plot to our simulated $p$--values can be seen in the figure.

When  the first $m_0$ null hypotheses are true and
the other $m-m_0$ are false, the cumulative distribution function of $(1-p_1,
\ldots, 1-p_{m_0})$ is  expected to be close to the line $F_0(t)=t$. The
cumulative distribution function of $(1-p_{m_0+1}, \ldots, 1-p_{m})$, on the
other hand, is expected to be close to a function $F_1(t)$ which stays below
$F_0$ but shows a steep increase towards 1 as $t$ approaches $1$.
In practice, we do not know which of the null hypotheses are true, so we can
only observe a mixture whose cumulative distribution function is expected to be
close to
%
\begin{equation*}
  F(t) = \frac{m_0}{m} F_0(t) + \frac{m-m_0}{m} F_1(t).
\end{equation*}
%
In our simulated data  $F_0 = 1-N(0,1)$ and $F_1 = 1-N(2,1)$. By looking
at the figure, the points start to deviate at 150 from the straight line, 
as expected from the simulation model.

<<plot SchwSpot plot >>=
(ggplot2::qplot(sort(1-pv), 1:200, xlab = "1 - p-values", ylab = "No. of hypotheses",
       main = "Schweder and Spotvoll plot")
 + geom_abline(intercept = 0, slope = 200*eta0.true, aes(color = "coral3")))
@

\subsubsection{Histogram of $p$--values}
As already mentioned, the $p$--values follow a uniform distribution on the unit interval
[0,1] if they are computed using a continuous null distribution. Significant $p$--values
thus become visible as an enrichment of $p$--values near zero in the histogram.
A histogram of $p$--values should always be plotted in order to check whether 
they have been computed correctly. 

<<pvalue histogram >>=
ggplot2::qplot(x = pv, xlab = "p-values", main = "Histogram of p-values, correct null 
        distribution", 
       fill = I("navyblue"))
@
We see that our $p$--values are uniformly distributed under the null hypotheses.
Computing the $p$--values assuming a $N(0,2)$ null distribution changes the picture.

<<pvalue histogram wrong null>>=
ggplot2::qplot(x = pnorm(z, sd = 2) , xlab = "p-values", main = "Histogram of p-values, 
      variance of null distribution too high", 
       fill = I("coral3"))
@

If the assumed variance of the null distribution is too high, we often see 
hill--shaped $p$--value histogram. If the variance is too low, we get a U--shaped
histogram, with peaks at both ends.

<<pvalue histogram wrong null 2 >>=
ggplot2::qplot(x = pnorm(z, sd = 0.5) , xlab = "p-values", main = "Histogram of p-values, 
      variance of null distribution too low", 
       fill = I("chartreuse4"))
@

\subsection{Computing multiple testing adjustments}
The most commonly used multiple testing adjustments can be computed using
the function \Rfunction{padjust}. To compute the Benjamini Hochberg
adjusted $p$--values simply specify \Rcode{ method = "BH"}. For FEWR
control we choose  \Rcode{ method = "bonferroni"}. 

<<p-value adjustments>>=
alpha = 0.05
pv.BH <- p.adjust(pv, method = "BH")
table(pv.BH < 0.05)
(ggplot2::qplot(rank(pv), pv, xlab = "p-value rank", ylab="p-values"
       ,main = "Visualization of the BH procedure")
+ geom_abline(intercept = 0, slope = alpha/200, aes(color = "coral3"))
+ ylim(c(0, 0.2) ))
pv.FWER <- p.adjust(pv, method = "bonferroni")
table(pv.FWER < 0.05)
@

The figure illustrates of the Benjamini-Hochberg multiple testing
adjustment. The black line shows the
$p$-values ($y$-axis) versus their rank ($x$-axis), starting with
the smallest $p$-value from the left, then the second smallest, and
so on. 
The red line is a straight line with slope $\alpha/m$, where
$m$ is the number of tests, and
$\alpha$ is a target false discovery rate.  FDR
is controlled at the value $\alpha$ if the genes are selected
that lie to the left of the rightmost intersection between the red and black
lines: here, this results in \Sexpr{sum(pv.BH < 0.05)} significant $p$--values.
Thus, the procedure is relatively conservative since we actually have simulated
25 non--null $p$--values. The Bonferroni correction is clearly not suitable here as
we only get 3 significant $p$--values.

\subsection{Modifying the BH procedure to gain power and the q--value}

The \CRANpkg{mutoss} package provides many more multiple testing adjustments. In
addition, it also has  the function \Rfunction{ABH\_ pi0\_ est}  that estimates
the proportion $\pi_0$ of the null model (in our case 75\%) for us based
on the Schweder and Spj\o{}tvoll plot. Here, 
it estimates $\pi_0$ as \Sexpr{mutoss::ABH_pi0_est(pv)}  which is quite far
away from the true value. However, with only 200 test statistics, it also
difficult to estimate $\pi_0$ reliably.

<<mutoss oracleBH>>=
 ABH_pi0_est(pv)
pv.OracleBH <- oracleBH(pValue=pv, alpha=alpha, pi0=0.75)
@

The BH procedure implicitly assumes $\pi_0 = 1$, i.e. that there are no 
non--null p-values in its original form. Thus,
we can gain power, by plugging in an estimate of $\pi_0$. Indeed, we gain
\Sexpr{sum(pv.OracleBH$rejected) - sum(pv.BH < alpha)}
more rejections.


The q--value is an FDR estimation procedure roughly defined as a BH procedure
combined with a $pi_0$ estimate --- pretty much like the oracle BH procedure above and is
very popular in  genomics. It tries to estimate $\pi_0$ from the $p$--value histogram. Note that it actually tries to estimate the FDR for a test statistic rather than just providing
a control of the FDR as the BH procedure.  

<<mutoss Qvalue>>=
pv.Qvals <- Qvalue(pv)
table(pv.Qvals$qValues < 0.05)
@ 
Although the number of $p$--values is low, the q--value procedure estimates $pi_0$
correctly and provides the same number of rejected hypotheses as oracle BH procedure.


\section{Regularized $t$--tests for small $n$, large $p$ problems}

In microarray analyses, on usually uses a variant of a (regularized) $t$--statistic
that is suitable for high-dimensional data and large-scale multiple testing such
as the one implemented in the Bioconductor package \Biocpkg{limma}. 

The basic statistic used for significance analysis in \Biocpkg{limma} is the moderated 
$t$--statistic, which is computed for each gene separately.
It has the same interpretation as an ordinary $t$--statistic except that 
the standard errors have been moderated across genes, i.e., shrunk toward a common value, 
using a simple Bayesian model. This has the effect of borrowing information from the 
ensemble of genes to aid with inference about each individual gene.
Moderated t-statistics lead to p- values in the same way that ordinary t-statistics do
except that the degrees of freedom are increased, reflecting the greater 
reliability associated with the smoothed standard errors.

\subsection{Some details of the \Biocpkg{limma} method}

The empirical Bayes method in  \Biocpkg{limma} assumes an inverse Chi-square prior
for the variance $\sigma^2$ with mean $s_0^2$ and degrees of freedom $f_0$. These
parameters are estimated from data and not set beforehand, hence this is an
"empirical" Bayesian method.

The posterior values for the residual variances are given by
\begin{gather*}
s_j^2 = \frac{f_0s_0^2 + f\sigma^2}{f_0 + f}
\end{gather*}
Where $f$ denotes the degrees of freedom for a gene. For two group comparison
$f =  n_1 + n_2 - 2$, where $n_1$  and  $n_2$ are the number of samples in each of the
groups.

\subsection{Shrinkage estimation}

The most important aspect of the \Biocpkg{limma} approach is that \Biocpkg{limma}
performs a SHRINKAGE of the variances towards a target, which is given by $s_0^2$, the 
prior mean variance and the shrinkage intensity is $\frac{f_0}{f_0 + f}$.

A $t$--test with $f_0 + f$ degrees of freedom using the shrunken variances
is  then computed to assess differential expression. There are many other
ways to perform shrinkage, which will commonly improve the estimation of the 
variance by sharing information across genes. A wide  selection of these
statistics is implemented in the package \CRANpkg{st} We use the \Rfunction{modt.stat} 
from the package  to compute \Biocpkg{limma}s
moderated $t$--statistic for BCL2 in the ALL data:


<<modTestBCL2,  echo = TRUE>>=

modt.stat(t(expALL), groupsALL)[1152]
### p-value using the normal distribution
2 - 2*pnorm(modt.stat(t(expALL), groupsALL)[1152])
### slightly higher t-value than the ordinary t-test
t.test(expALL[1152,] ~ groupsALL, var.equal=FALSE)$statistic
@

Since information about the overall variance of the genes in the data set is used
to compute the variance for CCND3, the whole data set has to be provided in
order to compute the moderated $t$--statistic.  

\subsubsection*{Exercise: Group comparison for gene GYPC}
The gene GYPC plays an important role in regulating the mechanical 
stability of red cells. I can be found
in line \Sexpr{grep("GYPC", anno_fusALL$SYMBOL)} of the expALL data set. (Try 
\Rfunction{grep("GYPC", anno\textunderscore fusALL\$SYMBOL))}. 

Test for the equality of the 
means by an appropriate $t$--test. Is the experimental effect very strong?
Also, try testing the hypothesis  using a moderated $t$--test and a wilcoxon
test.



\subsection{Multiple testing applied to the ALL data set}
We illustrate some multiple testing approaches with the expALL data set. For
this we use the shrinkage t statistic from the  \CRANpkg{st} package which
implements an analytical rather than a Bayesian shrinkage approach. First,
we compute the $t$--statistics and the associated tow--sided $p$--values using a standard
normal distribution as the null model. The resulting $p$--value histogram looks
good.

<<shrinkage t ALL>>=
sts <- shrinkt.stat(t(expALL), groupsALL)
### p-value using the normal distribution
pval.st <- 2 - 2*pnorm(abs(sts))
hist(pval.st, col = "lavender", main = "Histogram of p-values for 
     shrinkage t statistics of the ALL data")
@

We can compute a standard BH adjustment to obtain the number of differentially
expressed genes at an FDR of 5\%.

<<shrinkage t multiple testing>>=
pv.st.BH <- p.adjust(pval.st, method = "BH")
table(pv.st.BH< 0.05)

@


\subsubsection*{Exercise: Multiple testing for the ALL data}
Try other multiple testing procedures like q--values on the $p$--values 
obtained from the shrinkage t statistics. Can you gain power? Produce a $p$--value
histogram where significant statistics are indicated by color-fill.


\section{Answers to exercises}

\subsubsection*{Exercise: Normal Model for a gene} 

Suppose that the distribution of the expression values for a
gene is distributed according to $N(1.6, 0.42)$.

1. Compute the probability that the expression values are less
than 1.2.
2. What is the probability that the expression values are between 1.2
and 2.0?
3. What is the probability that the expression values are between 0.8
and 2.4?
4.  Compute the exact values for the quantiles $x_{0.025}$ and $x_{0.975}$.

5. Use \Rfunction{rnorm} to draw a sample of size 1000 from the population and
compare the sample mean and standard deviation to that of the population.


\subsubsection*{Solution:  Normal Model for a gene}

<< geneExp-Exercise, echo = TRUE, results ='hide' >>=
# a
#######################################################
1 - pnorm(1.2, 1.6, 0.42) 

# b 
####################################################### 
pnorm(2, 1.6, 0.42) - pnorm(1.2, 1.6, 0.42)

# c 
#######################################################   
pnorm(2.4, 1.6, 0.42)  - pnorm(0.8, 1.6, 0.42)


# d
#######################################################
qnorm(0.025, 1.6, 0.42)
qnorm(0.975, 1.6, 0.42)

# e
#######################################################
test.sample = rnorm(1000, 1.6, 0.42)
mean(test.sample)
sd(test.sample)
@


\xhrulefill{BiocBlue}{1pt}

\subsubsection*{Exercise: Rocky mountain spotted fever}

In 747 cases of "Rocky Mountain spotted fever" from the western United States, 
210 patients died. 
Out of 661 cases from the eastern United States, 
122 died. Is the difference statistically significant?
Use a prop--test as well as a Fisher--test.

\subsubsection*{Solution: Rocky mountain spotted fever}
<<sol-fever,   echo = TRUE, results='hide'>>=
deaths <-c(210,122)
tot.cases <-c(747,661)

prop.test(deaths, tot.cases)

chisq.test(rbind(deaths, tot.cases-deaths))
fisher.test(rbind(deaths, tot.cases-deaths))
@

\xhrulefill{BiocBlue}{1pt}

\subsubsection*{Exercise: Group comparison for gene GYPC}
The gene GYPC plays an important role in regulating the mechanical 
stability of red cells. I can be found
in line \Sexpr{grep("GYPC", anno_fusALL$SYMBOL)} of the expALL data set. (Try 
\Rfunction{grep("GYPC", anno\textunderscore fusALL\$SYMBOL))}. 

Test for the equality of the 
means by an appropriate $t$--test. Is the experimental effect very strong?
Also, try testing the hypothesis  using a moderated $t$--test and a wilcoxon
test.

\subsubsection*{Solution: Group comparison for gene GYPC}

<<sol-t-test,   echo = TRUE, results='hide'>>=

t.test(expALL[8197,] ~ groupsALL, var.equal=FALSE)

### strong difference between groups ...
### confirmed by wilcoxon test 
wilcox.test(expALL[8197,] ~ groupsALL)

### the moderated t-test is also significant 
modt.stat(t(expALL), groupsALL)[8197]
### p-value using the normal distribution
2 - 2*pnorm(abs(modt.stat(t(expALL), groupsALL)[8197]))
@


\xhrulefill{BiocBlue}{1pt}


\subsubsection*{Exercise: Multiple testing for the ALL data}
Try other multiple testing procedures like q--values on the $p$--values 
obtained from the shrinkage t statistics. Can you gain power? Produce a $p$--value
histogram where significant statistics are indicated by color-fill.

\subsubsection*{Solution: Multiple testing for the ALL data}
% results='hide
<<sol gol multiple testing, results='hide', fig.keep='none'>>=
golub.qval <- Qvalue(pval.st)
significant <- as.factor(golub.qval$qValues < 0.05)
levels(significant) <- ifelse(levels(significant) , "yes", "no")

table(significant)

ggplot2::qplot(pval.st , xlab = "p-values of shrinkage t statistics",
               main = "Histogram of p-values, 
       golub data, q-value < 0.05", 
       fill = significant)

significant <- as.factor(pv.st.BH < 0.05)
levels(significant) <- ifelse(levels(significant) , "yes", "no")

(ggplot2::qplot(pval.st , xlab = "p-values of shrinkage t statistics", 
                main = "Histogram of p-values, 
       golub data, BH adjusted p-value < 0.05", 
       fill = significant)  + scale_fill_brewer(type = "qual", palette = 8))
@


\end{document}
